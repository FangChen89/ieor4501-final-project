{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a622c55a",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "## IEOR E4501 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import distance\n",
    "import requests\n",
    "import re\n",
    "import sqlalchemy as db\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "from typing import List, Union, Any, Dict,  Tuple\n",
    "from matplotlib.container import BarContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "TAXI_DIR = \"data/taxi\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"data/uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"data/weather\"\n",
    "\n",
    "CRS = 4326 # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "    print(f\"Folder {QUERY_DIRECTORY} created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder {QUERY_DIRECTORY} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the TAXI_DIR exists\n",
    "try:\n",
    "    os.mkdir(TAXI_DIR)\n",
    "    print(f\"Folder {TAXI_DIR} created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder {TAXI_DIR} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the shapefile and return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shapefile : str\n",
    "        The relative path of the shape file including zone IDs and geometries.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "    \n",
    "    \"\"\"\n",
    "    loaded_taxi_zones = gpd.read_file(shapefile)\n",
    "    loaded_taxi_zones = loaded_taxi_zones[['OBJECTID', 'geometry']].set_index('OBJECTID')\n",
    "    # Transform geometries to the new coordinate reference system 4326\n",
    "    loaded_taxi_zones = loaded_taxi_zones.to_crs(CRS)\n",
    "    \n",
    "    return loaded_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id: int,\n",
    "                                   loaded_taxi_zones: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Given the zone ID and return the corresponding centroid coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zone_loc_id : int\n",
    "        The zone ID which needs to be searched.\n",
    "    loaded_taxi_zones : pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "\n",
    "    \"\"\"\n",
    "    geometry = loaded_taxi_zones.loc[zone_loc_id, 'geometry']\n",
    "    # Obtain the approximate coordinates by the centroid location\n",
    "    longitude = geometry.centroid.x\n",
    "    latitude = geometry.centroid.y\n",
    "    coords = (longitude, latitude)\n",
    "    \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord: tuple, to_coord: tuple) -> float:\n",
    "    \"\"\"Given the coordinates and return the distance between them.\n",
    "    \n",
    "    This function utilizes the Haversine formula to calculate the distance\n",
    "    between two coordinates on Earth's surface.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    from_coord : tuple of float\n",
    "        A tuple containing the longitude and latitude of the starting point, expressed in degrees.\n",
    "    to_coord : tuple of float\n",
    "        A tuple containing the longitude and latitude of the destination point, expressed in degrees.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The distance between the two coordinates, in kilometers.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input coordinates from degrees to radians\n",
    "    from_lon, from_lat = math.radians(from_coord[0]), math.radians(from_coord[1])\n",
    "    to_lon, to_lat = math.radians(to_coord[0]), math.radians(to_coord[1])\n",
    "    # Calculate the differences in latitude and longitude\n",
    "    delta_lon = to_lon - from_lon\n",
    "    delta_lat = to_lat - from_lat\n",
    "    # Apply the Haversine formula to calculate the distance\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(from_lat) * math.cos(to_lat) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = 6371 * c # earth's radius is assumed to be 6371 kilometers\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add the 'distance' column to the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the new calculated column 'distance'.\n",
    "    \n",
    "    \"\"\"\n",
    "    dataframe['distance'] = dataframe.apply(lambda x:calculate_distance_with_coords(\n",
    "                                (x['pickup_longitude'],x['pickup_latitude']),\n",
    "                                (x['dropoff_longitude'],x['dropoff_latitude'])), axis=1)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c0aeb",
   "metadata": {},
   "source": [
    "### Remove outside trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outside_trip(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove the trip records outside the defined region.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after removing all trip records outside the defined region.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Obtain the coordinate limits\n",
    "    southlimit, westlimit = NEW_YORK_BOX_COORDS[0]\n",
    "    northlimit, eastlimit = NEW_YORK_BOX_COORDS[1]\n",
    "    # Remove the trips outside the location \n",
    "    df = df[(df['pickup_longitude'] >= westlimit) & (df['pickup_longitude'] <= eastlimit)]\n",
    "    df = df[(df['pickup_latitude'] >= southlimit) & (df['pickup_latitude'] <= northlimit)]\n",
    "\n",
    "    df = df[(df['dropoff_longitude'] >= westlimit) & (df['dropoff_longitude'] <= eastlimit)]\n",
    "    df = df[(df['dropoff_latitude'] >= southlimit) & (df['dropoff_latitude'] <= northlimit)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page: str) -> list:\n",
    "    \"\"\"Scrap the URLs from the given page and return them as a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    taxi_page : str\n",
    "        The URL of the target page.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of URLs scraped from the given page.\n",
    "        \n",
    "    \"\"\"\n",
    "    all_urls = list()\n",
    "    \n",
    "    content = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(content.text, 'lxml')\n",
    "    # Find all the URLs in the page\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        all_urls.append(link.get('href'))\n",
    "        \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_parquet_urls(all_urls: list) -> list:\n",
    "    \"\"\"Find the URLs for yellow taxi data and return them as a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_urls : str\n",
    "        A list of URLs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of URLs for yellow taxi parquet files from 2019-01 to 2015-06.\n",
    "        \n",
    "    \"\"\"\n",
    "    all_parquet_urls = list()\n",
    "    \n",
    "    pattern = r\".*yellow_tripdata.*parquet\\Z\"\n",
    "    time_pattern = r\"(2009-(0[1-9]|1[0-2]))|(201[0-4]-(0[1-9]|1[0-2]))|(2015-(0[1-6]))\"\n",
    "    for url in all_urls:\n",
    "        # Check if the URL belongs to yellow taxi trip data\n",
    "        if re.search(pattern, url):\n",
    "            # Check if the URL belongs to the time range for the project\n",
    "            if re.search(time_pattern, url):\n",
    "                all_parquet_urls.append(url)\n",
    "            \n",
    "    return all_parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coords_from_zones(dataframe):\n",
    "    \"\"\"Generate the coordinates from zone IDs in a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after transforming zone IDs to longitude and latitude coordinates.\n",
    "    \n",
    "    \"\"\"\n",
    "    loaded_taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    southlimit, westlimit = NEW_YORK_BOX_COORDS[0]\n",
    "    northlimit, eastlimit = NEW_YORK_BOX_COORDS[1]\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        pickup_zoneid = row['pickup_zoneid']\n",
    "        dropoff_zoneid = row['dropoff_zoneid']\n",
    "\n",
    "        pickup_coords = lookup_coords_for_taxi_zone_id(pickup_zoneid, loaded_taxi_zones)\n",
    "        \n",
    "        # define the initial bearing\n",
    "        direction = 0\n",
    "        # check if pickup and dropoff zones are the same\n",
    "        if pickup_zoneid == dropoff_zoneid:\n",
    "            # generate dropoff coordinates using distance and bearing\n",
    "            dropoff_coords = distance(\n",
    "                miles=row['trip_distance']).destination(pickup_coords[::-1], bearing=direction)[1::-1]\n",
    "        else:\n",
    "            # generate dropoff coordinates using dropoff zone ID\n",
    "            dropoff_coords = lookup_coords_for_taxi_zone_id(dropoff_zoneid, loaded_taxi_zones)\n",
    "        \n",
    "        # check if dropoff coordinates fall outside the defined box\n",
    "        while not ((westlimit <= dropoff_coords[0] <= eastlimit) and \n",
    "                   (southlimit <= dropoff_coords[1] <= northlimit)):\n",
    "            # Generate new dropoff coordinates by changing the bearing\n",
    "            direction += 90\n",
    "            # If all four bearings do not work, drop this record instead\n",
    "            if direction == 360:\n",
    "                break\n",
    "            dropoff_coords = distance(\n",
    "                miles=row['trip_distance']).destination(pickup_coords, bearing=direction)[1::-1]\n",
    "        \n",
    "        # update the dataframe with the generated coordinates\n",
    "        if direction != 360:\n",
    "            dataframe.loc[index, 'pickup_longitude'] = pickup_coords[0]\n",
    "            dataframe.loc[index, 'pickup_latitude'] = pickup_coords[1]\n",
    "            dataframe.loc[index, 'dropoff_longitude'] = dropoff_coords[0]\n",
    "            dataframe.loc[index, 'dropoff_latitude'] = dropoff_coords[1]\n",
    "        else:\n",
    "            dataframe.drop(index=index, inplace=True)\n",
    "    \n",
    "    # Drop the unnecessary columns\n",
    "    dataframe.drop(['trip_distance', 'pickup_zoneid', 'dropoff_zoneid'], axis=1, inplace=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_df_2009_to_2010(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the yellow taxi data from 2009 to 2010.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe with records from 2009 to 2010.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after normalizing column names, removing invalid data, and sampling.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Normalize the column names\n",
    "    dataframe.columns = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "                         'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                         'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', \n",
    "                         'payment_type', 'fare_amount', 'surcharge', 'mta_tax', 'tip_amount', \n",
    "                         'tolls_amount', 'total_amount']\n",
    "    \n",
    "    # Remove the trips outside the required coordinate box\n",
    "    dataframe = remove_outside_trip(dataframe)\n",
    "    # Remove the trips with zero passenger count\n",
    "    dataframe = dataframe[dataframe['passenger_count'] != 0]\n",
    "    # Remove the trips without a fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] != 0]\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[dataframe['trip_distance'] != 0]\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] != dataframe['dropoff_longitude']) \n",
    "                          & (dataframe['pickup_latitude'] != dataframe['dropoff_latitude'])]\n",
    "    \n",
    "    # Sample the taxi data at a appropriate number\n",
    "    dataframe = dataframe.sample(n = 2500, random_state=1)\n",
    "    \n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "                       'dropoff_longitude', 'dropoff_latitude', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "    \n",
    "    # Transform the pickup datetime column from strings to datetime\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'],\n",
    "                                                  format='%Y-%m-%d %H:%M:%S') \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f900c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_df_2011_to_2015(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the yellow taxi data from 2011 to 2015.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe with records from 2011 to 2015.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after normalizing column names, removing invalid data, and sampling.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Normalize the column names\n",
    "    dataframe.columns = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "                         'trip_distance', 'rate_code', 'store_and_fwd_flag', 'pickup_zoneid',\n",
    "                         'dropoff_zoneid', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax',\n",
    "                         'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "                         'congestion_surcharge', 'airport_fee']\n",
    "    \n",
    "    # Remove the trips outside the 1-263 zones\n",
    "    dataframe = dataframe[dataframe['pickup_zoneid'] != 264]\n",
    "    dataframe = dataframe[dataframe['pickup_zoneid'] != 265]\n",
    "    dataframe = dataframe[dataframe['dropoff_zoneid'] != 264]\n",
    "    dataframe = dataframe[dataframe['dropoff_zoneid'] != 265]\n",
    "    # Remove the trips with zero passenger count\n",
    "    dataframe = dataframe[dataframe['passenger_count'] != 0]\n",
    "    # Remove the trips without a fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] != 0]\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[dataframe['trip_distance'] != 0]\n",
    "    \n",
    "    # Sample the taxi data at a appropriate number\n",
    "    dataframe = dataframe.sample(n = 2500, random_state=1)\n",
    "    \n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['pickup_datetime', 'trip_distance',\n",
    "                       'pickup_zoneid', 'dropoff_zoneid', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "    \n",
    "    # Generate the coordinates from zone IDs\n",
    "    dataframe = generate_coords_from_zones(dataframe)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean the parquet file for the URL, return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL for the parquet file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after loading and cleaning the parquet file from the given URL.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Programmatically download needed data if not exists\n",
    "    dataframe = pd.DataFrame()\n",
    "    \n",
    "    time_pattern = r\"(2009-(0[1-9]|1[0-2]))|(201[0-4]-(0[1-9]|1[0-2]))|(2015-(0[1-6]))\"\n",
    "    time = \"\"\n",
    "    \n",
    "    if re.search(time_pattern, url):\n",
    "        time = re.search(time_pattern, url).group(0)\n",
    "        file_path = f\"{TAXI_DIR}/yellow_taxi_{time}.parquet\"\n",
    "        \n",
    "        # Check if the parquet file has already been downloaded\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File {file_path} already exists.\")\n",
    "            dataframe = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        else:\n",
    "            # If not, download the file from the given URL\n",
    "            print(f\"File {file_path} does not exist. Downloading...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                print(f\"File {file_path} downloaded successfully.\")\n",
    "            dataframe = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    \n",
    "    # Clean the dataframe\n",
    "    if re.search(r\"2009|2010\", time):\n",
    "        dataframe_cleaned = clean_taxi_df_2009_to_2010(dataframe)\n",
    "    else:\n",
    "        dataframe_cleaned = clean_taxi_df_2011_to_2015(dataframe)\n",
    "    \n",
    "    return dataframe_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls: list) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess and concatenate all the data, return them as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parquet_urls : list\n",
    "        A list of URLs for parquet files of yellow taxi data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after preprocessing and concatenating all the parquet file data.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    # Iterate the URLs and obtain the dataframe for each month\n",
    "    for parquet_url in parquet_urls:\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # Add the 'distance' column\n",
    "        dataframe = add_distance_column(dataframe)\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "\n",
    "    # Create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() -> pd.DataFrame:\n",
    "    \"\"\"Scrap the yellow taxi data and return the result as a DataFrame.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including all cleaned and sampled records for yellow taxi data.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sampled yellow taxi data as a CSV file\n",
    "taxi_data.to_csv('data/taxi/yellow_taxi_sampled.csv', index=False)\n",
    "# Load the sampled yellow taxi data directly\n",
    "taxi_data = pd.read_csv('data/taxi/yellow_taxi_sampled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean the Uber data and return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_file : str\n",
    "        The relative path of the CSV file of Uber data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after loading and cleaning all the Uber data.\n",
    "    \n",
    "    \"\"\"\n",
    "    columns_to_keep = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "                       'dropoff_longitude', 'dropoff_latitude']\n",
    "    dataframe = pd.read_csv(csv_file, usecols = columns_to_keep)\n",
    "    # Transform the pickup datetime column from strings to datetime\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'],\n",
    "                                                  format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    dataframe['pickup_datetime'] = dataframe['pickup_datetime'].dt.tz_convert(None)\n",
    "    \n",
    "    # Remove the trips outside the defined coordinate box\n",
    "    dataframe = remove_outside_trip(dataframe)\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] != dataframe['dropoff_longitude']) \n",
    "                          & (dataframe['pickup_latitude'] != dataframe['dropoff_latitude'])]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame:\n",
    "    \"\"\"Return the processed Uber data as a DataFrame.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after preprocessing all the Uber data and adding column 'distance'.\n",
    "    \n",
    "    \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    # Add the 'distance' column\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_invalid_rows_for_HourlyWindSpeed(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows from the input dataframe if the DATE column value is not 23:59\n",
    "    and the HourlyWindSpeed column value is not a float or is NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Input dataframe with 'DATE' and 'HourlyWindSpeed' columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataframe after dropping invalid rows.\n",
    "    \"\"\"\n",
    "    # Filter rows with DATE values not equal to 23:59\n",
    "    not_2359_rows = (dataframe['DATE'].dt.hour != 23) | (dataframe['DATE'].dt.minute != 59)\n",
    "    \n",
    "    # Check if the HourlyWindSpeed is of type float and not NaN\n",
    "    is_float_and_not_nan = dataframe['HourlyWindSpeed'].apply(lambda x: isinstance(x, float) and not pd.isna(x))\n",
    "    \n",
    "    # Combine both conditions using bitwise AND\n",
    "    invalid_rows = not_2359_rows & ~is_float_and_not_nan\n",
    "\n",
    "    # Drop the invalid rows\n",
    "    dataframe = dataframe[~invalid_rows]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_missing_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a DataFrame containing missing daily records.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing weather data from January 2009 to June 2015.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with missing daily records.\n",
    "    \"\"\"\n",
    "    # Find the minimum and maximum dates in the input DataFrame\n",
    "    min_date = df['DATE'].min().normalize()\n",
    "    max_date = df['DATE'].max().normalize()\n",
    "    # Create a date range from the minimum to maximum date\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    # Generate missing daily records by checking if a record exists for each date\n",
    "    missing_dates = [\n",
    "        {'DATE': date.replace(hour=23, minute=59, second=0)}\n",
    "        for date in date_range\n",
    "        if not ((df['DATE'] == date.replace(hour=23, minute=59, second=0)).any())\n",
    "    ]\n",
    "    return pd.DataFrame(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_daily_wind_speed(new_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Fill missing daily wind speed values by calculating the mean hourly wind speed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    new_df : pd.DataFrame\n",
    "        The input DataFrame containing weather data with missing daily wind speed values.\n",
    "    \"\"\"\n",
    "    # Iterate through the rows of the DataFrame with missing daily wind speed values\n",
    "    for index, row in new_df.loc[new_df['DATE'].dt.strftime('%H:%M') == '23:59'].iterrows():\n",
    "        if pd.isna(row['DailyAverageWindSpeed']):\n",
    "            date = row['DATE'].date()\n",
    "            # Calculate the mean hourly wind speed for the current date\n",
    "            hourly_wind_speed_mean = new_df.loc[\n",
    "                (new_df['DATE'].dt.date == date) & (new_df['DATE'] != row['DATE']) & (~new_df['HourlyWindSpeed'].isna()),\n",
    "                'HourlyWindSpeed'\n",
    "            ].mean()\n",
    "            # Replace the missing daily wind speed value with the calculated mean\n",
    "            new_df.loc[index, 'DailyAverageWindSpeed'] = hourly_wind_speed_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean the CSV files, return them as a list of DataFrames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        The directory path containing the CSV files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[pd.DataFrame]\n",
    "        A list of cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    # List all CSV files in the directory\n",
    "    csv_files = [entry.name for entry in os.scandir(directory)\n",
    "        if entry.name.endswith('.csv') and entry.is_file()]\n",
    "    dfs = []\n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = [\n",
    "        'DATE', 'HourlyPrecipitation', 'HourlyWindSpeed',\n",
    "        'DailyAverageWindSpeed', 'Sunrise', 'Sunset'\n",
    "    ]\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Join the directory path and CSV file name to form the full file path\n",
    "        file_path = os.path.join(directory, csv_file)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, usecols=columns_to_keep, parse_dates=['DATE'], engine='python')\n",
    "        \n",
    "        # Filter the DataFrame by date range\n",
    "        df = df[(df['DATE'] >= '2009-01-01 00:00:00') & (df['DATE'] <= '2015-06-30 23:59:59')]\n",
    "        \n",
    "        # Drop rows that do not meet the requirement of 'HourlyWindSpeed'.\n",
    "        df = drop_invalid_rows_for_HourlyWindSpeed(df)\n",
    "        \n",
    "        # Generate a DataFrame with missing dates\n",
    "        missing_df = generate_missing_dates(df)\n",
    "        \n",
    "        # Combine the original DataFrame and the missing dates DataFrame\n",
    "        new_df = pd.concat([df, missing_df], ignore_index=True)\n",
    "        # Sort the new DataFrame by date\n",
    "        new_df.sort_values(by='DATE', inplace=True)\n",
    "        \n",
    "        # Fill in missing daily wind speed values\n",
    "        fill_missing_daily_wind_speed(new_df)\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list of DataFrames\n",
    "        dfs.append(new_df)\n",
    "        \n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hourly_precipitation(hourly_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the 'HourlyPrecipitation' column of the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hourly_data : pd.DataFrame\n",
    "        Input DataFrame containing hourly weather data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing hourly data.\n",
    "    \"\"\"\n",
    "    # Remove 's' from the HourlyPrecipitation values, replace 'T' with 0, and fill missing values with 0\n",
    "    hourly_data['HourlyPrecipitation'] = hourly_data['HourlyPrecipitation'].str.replace('s', '')\n",
    "    hourly_data['HourlyPrecipitation'] = hourly_data['HourlyPrecipitation'].replace('T', 0)\n",
    "    hourly_data['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    # Convert the HourlyPrecipitation column to numeric type\n",
    "    hourly_data['HourlyPrecipitation'] = pd.to_numeric(hourly_data['HourlyPrecipitation'])\n",
    "    return hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_hourly_data(hourly_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate the input DataFrame's data to remove multiple reports within an hour.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hourly_data : pd.DataFrame\n",
    "        Input DataFrame containing hourly weather data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with aggregated hourly data.\n",
    "    \"\"\"\n",
    "    # Round the datetime to the nearest hour\n",
    "    hourly_data['date'] = hourly_data['date'].dt.floor('H')\n",
    "    # Group by hour and aggregate the data by taking the maximum of hourly_precipitation and hourly_windspeed\n",
    "    return hourly_data.groupby('date').agg(\n",
    "        {'hourly_precipitation': 'max', 'hourly_windspeed': 'max'}\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the DataFrame and return only hourly weather data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_file : pd.DataFrame\n",
    "        Input DataFrame containing weather data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with aggregated hourly data.\n",
    "    \"\"\"\n",
    "    # Split the input DataFrame into daily and hourly data\n",
    "    daily_data = csv_file[(csv_file['DATE'].dt.hour == 23) & (csv_file['DATE'].dt.minute == 59)]\n",
    "    hourly_data = csv_file.drop(daily_data.index)\n",
    "    # Select only relevant columns for further processing\n",
    "    columns_to_keep = ['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']\n",
    "    hourly_data = hourly_data[columns_to_keep]\n",
    "\n",
    "    # Clean the hourly precipitation and wind speed data\n",
    "    hourly_data = clean_hourly_precipitation(hourly_data)\n",
    "    # Clean the 'HourlyWindSpeed' column\n",
    "    hourly_data['HourlyWindSpeed'].fillna(0, inplace=True)\n",
    "    # Normalize the column names\n",
    "    hourly_data.columns = ['date', 'hourly_precipitation', 'hourly_windspeed']\n",
    "    # Aggregate the hourly data to remove multiple reports within an hour\n",
    "    result = aggregate_hourly_data(hourly_data)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_time_string(value: float) -> str:\n",
    "    \"\"\"\n",
    "    Transform the float value to time string in HH:MM format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    value : float\n",
    "        Float value representing time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Formatted time string in HH:MM format.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    else:\n",
    "        # Extract hours and minutes from the float value\n",
    "        hours = int(value // 100)\n",
    "        minutes = int(value % 100)\n",
    "    # Return formatted time string\n",
    "    return f\"{hours:02d}:{minutes:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_datetime(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert 'Sunrise' and 'Sunset' time strings to datetime objects using the date from the 'DATE' column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : Dict[str, Any]\n",
    "        A dictionary representing a row in the DataFrame containing 'DATE', 'Sunrise', and 'Sunset' columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        The modified row with 'Sunrise' and 'Sunset' columns converted to datetime objects.\n",
    "    \"\"\"\n",
    "    # Extract the date from the 'DATE' column\n",
    "    date = row['DATE'].date()\n",
    "    \n",
    "    # Check if the 'Sunrise' value is not None\n",
    "    if row['Sunrise'] is not None:\n",
    "        # Convert the 'Sunrise' time string to a time object\n",
    "        sunrise_time = pd.to_datetime(row['Sunrise'], format=\"%H:%M\").time()\n",
    "        # Combine the date and sunrise_time to create a datetime object\n",
    "        row['Sunrise'] = pd.to_datetime(f\"{date} {sunrise_time}\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Check if the 'Sunset' value is not None\n",
    "    if row['Sunset'] is not None:\n",
    "        # Convert the 'Sunset' time string to a time object\n",
    "        sunset_time = pd.to_datetime(row['Sunset'], format=\"%H:%M\").time()\n",
    "        # Combine the date and sunset_time to create a datetime object\n",
    "        row['Sunset'] = pd.to_datetime(f\"{date} {sunset_time}\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Return the modified row\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the DataFrame and return only daily weather data.\n",
    "\n",
    "    The function processes the input DataFrame and returns a cleaned DataFrame\n",
    "    with daily weather data. The steps include extracting daily data, selecting\n",
    "    relevant columns, cleaning sunrise and sunset data, and normalizing column names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_file : pd.DataFrame\n",
    "        DataFrame containing weather data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing cleaned daily weather data.\n",
    "    \"\"\"\n",
    "    # Filter daily data based on timestamp (hour=23 and minute=59)\n",
    "    daily_data = csv_file[(csv_file['DATE'].dt.hour == 23) & (csv_file['DATE'].dt.minute == 59)]\n",
    "    # Define the columns to keep\n",
    "    columns_to_keep = ['DATE', 'DailyAverageWindSpeed', 'Sunrise', 'Sunset']\n",
    "    daily_data = daily_data[columns_to_keep]\n",
    "    \n",
    "    # Clean the  \"Sunrise\" and \"Sunset\" columns\n",
    "    # Convert 'Sunrise' and 'Sunset' columns to strings in HH:MM format\n",
    "    daily_data['Sunrise'] = daily_data['Sunrise'].apply(float_to_time_string)\n",
    "    daily_data['Sunset'] = daily_data['Sunset'].apply(float_to_time_string)\n",
    "    # Convert 'Sunrise' and 'Sunset' columns to datetime\n",
    "    daily_data = daily_data.apply(convert_time_to_datetime, axis=1)\n",
    "    \n",
    "    # Normalize the column names\n",
    "    daily_data.columns = ['date', 'daily_average_windspeed', 'sunrise', 'sunset']\n",
    "    # Round the datetime column in a DataFrame to the nearest day.\n",
    "    daily_data[\"date\"] = pd.to_datetime(daily_data[\"date\"]).dt.date\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4444eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Load and clean weather data, return hourly and daily records as DataFrames.\n",
    "\n",
    "    The function loads weather data from CSV files, cleans the data, and returns\n",
    "    two DataFrames containing hourly and daily weather data respectively.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple containing two DataFrames: (hourly_data, daily_data).\n",
    "    \"\"\"\n",
    "    # Load weather CSV files\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "\n",
    "    # Process each CSV file\n",
    "    for csv_file in weather_csv_files:\n",
    "        # Clean hourly and daily weather data\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        # Append DataFrames to respective lists\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "\n",
    "    # Create two DataFrames with hourly and daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "\n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table schema\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_windspeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATETIME,\n",
    "    daily_average_windspeed FLOAT,\n",
    "    sunrise DATETIME,\n",
    "    sunset DATETIME\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict: Dict[str, pd.DataFrame]) -> None:\n",
    "    \"\"\"\n",
    "    Write DataFrames to their corresponding database tables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_to_df_dict : Dict[str, pd.DataFrame]\n",
    "        A dictionary where keys represent table names and values represent DataFrames to be written to those tables.\n",
    "    \"\"\"\n",
    "    # Iterate through the table name and DataFrame pairs in the dictionary\n",
    "    for table, dataframe in table_to_df_dict.items():\n",
    "        # Write the DataFrame to the corresponding table in the database\n",
    "        dataframe.to_sql(table, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query: str, outfile: str) -> None:\n",
    "    \"\"\"Helper function to write the queries to file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        The query statement.\n",
    "    outfile : str\n",
    "        The relative path of the output file.\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = f\"{QUERY_DIRECTORY}/taxi-popularity-by-hour.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%H', pickup_datetime), \n",
    "    COUNT(*) AS Count\n",
    "FROM\n",
    "    taxi_trips\n",
    "GROUP BY \n",
    "    strftime('%H', pickup_datetime)\n",
    "ORDER BY \n",
    "    Count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce003f9a",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c25006",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"uber_trips_weekday_popularity.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT\n",
    "    CASE\n",
    "        WHEN strftime('%w', pickup_datetime) = '0' THEN 'Sunday'\n",
    "        WHEN strftime('%w', pickup_datetime) = '1' THEN 'Monday'\n",
    "        WHEN strftime('%w', pickup_datetime) = '2' THEN 'Tuesday'\n",
    "        WHEN strftime('%w', pickup_datetime) = '3' THEN 'Wednesday'\n",
    "        WHEN strftime('%w', pickup_datetime) = '4' THEN 'Thursday'\n",
    "        WHEN strftime('%w', pickup_datetime) = '5' THEN 'Friday'\n",
    "        WHEN strftime('%w', pickup_datetime) = '6' THEN 'Saturday'\n",
    "    END AS weekday,\n",
    "    COUNT(*) AS popularity\n",
    "FROM\n",
    "    uber_trips\n",
    "GROUP BY\n",
    "    weekday\n",
    "ORDER BY\n",
    "    popularity DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21befd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44642474",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d124d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = f\"{QUERY_DIRECTORY}/trip_distance_95_percentile_july_2013.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH all_trips AS (SELECT pickup_datetime, distance FROM taxi_trips\n",
    "                   UNION ALL\n",
    "                   SELECT pickup_datetime, distance FROM uber_trips)\n",
    "SELECT\n",
    "    distance\n",
    "FROM\n",
    "    all_trips\n",
    "WHERE\n",
    "    strftime('%Y', pickup_datetime) = '2013' AND strftime('%m', pickup_datetime) = '07'\n",
    "ORDER BY\n",
    "    distance\n",
    "lIMIT 1\n",
    "OFFSET (SELECT COUNT(*)\n",
    "        FROM all_trips\n",
    "        WHERE strftime('%Y', pickup_datetime) = '2013' AND strftime('%m', pickup_datetime) = '07') * 95 / 100 - 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1034e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51764128",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00061f8f",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef94d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"top_10_busiest_days_with_avg_distance_2009.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "SELECT\n",
    "    date(pickup_datetime) AS pickup_date,\n",
    "    COUNT(*) AS total_rides,\n",
    "    AVG(distance) AS avg_distance\n",
    "FROM\n",
    "    (\n",
    "        SELECT pickup_datetime, distance\n",
    "        FROM uber_trips\n",
    "        WHERE strftime('%Y', pickup_datetime) = '2009'\n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime, distance\n",
    "        FROM taxi_trips\n",
    "        WHERE strftime('%Y', pickup_datetime) = '2009'\n",
    "    )\n",
    "GROUP BY\n",
    "    pickup_date\n",
    "ORDER BY\n",
    "    total_rides DESC, pickup_date\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9202cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b03583",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00dce58",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = f\"{QUERY_DIRECTORY}/windiest_10_days_with_num_of_trips.sql\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "WITH all_trips AS (SELECT pickup_datetime, distance FROM taxi_trips\n",
    "                   UNION ALL\n",
    "                   SELECT pickup_datetime, distance FROM uber_trips)\n",
    "SELECT\n",
    "    w.date, w.daily_average_windspeed, COUNT(*) as num_trips\n",
    "FROM\n",
    "    daily_weather AS w\n",
    "    JOIN\n",
    "    all_trips AS t ON strftime('%Y-%m-%d', w.date) = strftime('%Y-%m-%d', t.pickup_datetime)\n",
    "WHERE\n",
    "    w.date BETWEEN '2014-01-01' AND '2014-12-31'\n",
    "GROUP BY\n",
    "    w.date\n",
    "ORDER BY\n",
    "    w.daily_average_windspeed DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b51582",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e971b1d",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = \"hurricane_sandy_hourly_trips_and_weather.sql\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "WITH RECURSIVE\n",
    "    date_range (hour, n) AS (\n",
    "        SELECT datetime(strftime('%Y-%m-%d %H', '2012-10-22 00:00:00', '+0 hours') || ':00:00') AS hour, 0 AS n\n",
    "        UNION ALL\n",
    "        SELECT datetime(strftime('%Y-%m-%d %H', '2012-10-22 00:00:00', '+' || (n + 1) || ' hours') || ':00:00') AS hour, n + 1\n",
    "        FROM date_range\n",
    "        WHERE n < 383\n",
    "    ),\n",
    "    trips AS (\n",
    "        SELECT \n",
    "            datetime(strftime('%Y-%m-%d %H', pickup_datetime) || ':00:00') AS pickup_hour,\n",
    "            COUNT(*) AS trip_count\n",
    "        FROM (\n",
    "            SELECT pickup_datetime FROM uber_trips\n",
    "            UNION ALL\n",
    "            SELECT pickup_datetime FROM taxi_trips\n",
    "        )\n",
    "        WHERE pickup_datetime BETWEEN '2012-10-22 00:00:00' AND '2012-11-06 23:59:59'\n",
    "        GROUP BY pickup_hour\n",
    "    )\n",
    "SELECT\n",
    "    dr.hour AS date_hour_range,\n",
    "    COALESCE(t.trip_count, 0) AS trip_count,\n",
    "    COALESCE(hw.hourly_precipitation, 0.0) AS hourly_precipitation,\n",
    "    COALESCE(hw.hourly_windspeed, 0.0) AS hourly_windspeed\n",
    "FROM\n",
    "    date_range dr\n",
    "LEFT JOIN\n",
    "    trips t ON dr.hour = t.pickup_hour\n",
    "LEFT JOIN\n",
    "    hourly_weather hw ON dr.hour = datetime(strftime('%Y-%m-%d %H', hw.date) || ':00:00')\n",
    "ORDER BY\n",
    "    dr.hour;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbac1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05db72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c424a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_std(df: pd.DataFrame, column: str, threshold: float = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove the outliers from the input dataframe based on the standard deviation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    column : str\n",
    "        The column name to remove outliers from.\n",
    "    threshold : float, optional\n",
    "        The number of standard deviations to use as the threshold. Defaults to 3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataframe without outliers.\n",
    "    \"\"\"\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "\n",
    "    lower_bound = mean - threshold * std\n",
    "    upper_bound = mean + threshold * std\n",
    "\n",
    "    df_no_outliers = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    \n",
    "    return df_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_taxi_by_hour(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot the popularity of Yellow Taxi rides by each hour.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataframe containing data with the hour and number of taxi rides.\n",
    "    \n",
    "    \"\"\"\n",
    "    figure, axes = plt.subplots(figsize=(18, 8))\n",
    "    \n",
    "    values = dataframe['Count']\n",
    "    \n",
    "    axes.bar(dataframe['Hour'], values)\n",
    "    axes.set_title(\"Popularity of Yellow Taxi rides for each hour of the day\")\n",
    "    axes.set_xlabel(\"Hour\")\n",
    "    axes.set_ylabel(\"Number of Yellow Taxi Rides\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_num_taxi_by_hour() -> pd.DataFrame:\n",
    "    \"\"\"Obtain the popularity of Yellow Taxi rides by each hour.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including hours and corresponding number of yellow taxi rides.\n",
    "    \n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            strftime('%H', pickup_datetime) AS Hour, \n",
    "            COUNT(*) AS Count\n",
    "        FROM\n",
    "            taxi_trips\n",
    "        GROUP BY \n",
    "            strftime('%H', pickup_datetime);\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_sql_query(query, engine)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_taxi_by_hour = data_num_taxi_by_hour()\n",
    "plot_num_taxi_by_hour(df_num_taxi_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3046562",
   "metadata": {},
   "source": [
    "### Visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8705628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_statistics(dataframe: pd.DataFrame) -> Tuple[pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"Calculate monthly statistics, including mean, standard deviation, and confidence interval.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        DataFrame containing distance data with a 'month' column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple containing mean distances, standard deviations, and confidence intervals (means, std_devs, ci)\n",
    "    \"\"\"\n",
    "    df = dataframe.sort_values(by='month')\n",
    "    means = df.groupby('month')['distance'].mean()\n",
    "    std_devs = df.groupby('month')['distance'].std()\n",
    "    ci = 1.645 * (std_devs / np.sqrt(df.groupby('month')['distance'].count()))\n",
    "    \n",
    "    return means, std_devs, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30381d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_labels(ax: Axes, means: pd.Series, ci: pd.Series) -> None:\n",
    "    \"\"\"Set plot labels for the bar plot, including x-axis, y-axis, and title.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : Axes\n",
    "        Axes object of the bar plot.\n",
    "    means : pd.Series\n",
    "        Mean distances for each month.\n",
    "    ci : pd.Series\n",
    "        Confidence intervals for each month.\n",
    "    \"\"\"\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Average Distance (km)')\n",
    "    ax.set_title('The average distance of trips for each month\\n(2009.1-2015.6) with 90% confidence intervals')\n",
    "    ax.set_ylim(means.min() - ci.max() - 0.05, means.max() + ci.max() * 1.5)\n",
    "    ax.yaxis.set_ticks(np.arange(means.min() - ci.max(), means.max() + ci.max() * 1.5, 0.1))\n",
    "    \n",
    "    # Set custom x-axis labels\n",
    "    month_labels = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    ax.set_xticks(range(len(month_labels)))\n",
    "    ax.set_xticklabels(month_labels, rotation=0)  # Set rotation to 0 for horizontal labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bar_labels(ax: Axes, bars: BarContainer, means: pd.Series, ci: pd.Series) -> None:\n",
    "    \"\"\"Add labels to the bars in the bar plot, including bar height and confidence interval limits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : Axes\n",
    "        Axes object of the bar plot.\n",
    "    bars : BarContainer\n",
    "        Bars of the bar plot.\n",
    "    means : pd.Series\n",
    "        Mean distances for each month.\n",
    "    ci : pd.Series\n",
    "        Confidence intervals for each month.\n",
    "    \"\"\"\n",
    "    for i, bar in enumerate(bars.patches):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, height - 0.02, f\"{height:.2f}\", ha='center', va='top', fontsize=8, color='white')\n",
    "\n",
    "        upper_limit = means.iloc[i] + ci.iloc[i]\n",
    "        lower_limit = means.iloc[i] - ci.iloc[i]\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, upper_limit + 0.005, f\"{upper_limit:.2f}\", ha='center', va='bottom', fontsize=8, color='black')\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, lower_limit - 0.005, f\"{lower_limit:.2f}\", ha='center', va='top', fontsize=8, color='black')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d258be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_trip_distance_per_month(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"Plots a bar chart showing the average trip distance per month, based on the data in the given DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        The DataFrame containing trip distance data. It should have at least two columns: 'month' and 'distance'.\n",
    "    \"\"\"\n",
    "   # Calculate means, std_devs, and ci\n",
    "    means, std_devs, ci = calculate_monthly_statistics(dataframe)\n",
    "\n",
    "    # Create a bar plot with error bars representing confidence intervals\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    bars = means.plot(kind='bar', yerr=ci, capsize=4, ax=ax)\n",
    "\n",
    "    set_plot_labels(ax, means, ci)\n",
    "    add_bar_labels(ax, bars, means, ci)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_2() -> pd.DataFrame:\n",
    "    \"\"\"Retrieve the data for plotting average trip distance per month.\n",
    "    \"\"\"\n",
    "    QUERY = \"\"\"\n",
    "    SELECT strftime('%m', pickup_datetime) as month, distance as distance\n",
    "    FROM (\n",
    "      SELECT pickup_datetime, distance FROM taxi_trips\n",
    "      UNION ALL\n",
    "      SELECT pickup_datetime, distance FROM uber_trips\n",
    "    ) as combined_trips;\n",
    "    \"\"\"\n",
    "    result = engine.execute(QUERY).fetchall()\n",
    "    df = pd.DataFrame(result, columns=['month', 'distance'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_2()\n",
    "plot_avg_trip_distance_per_month(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c8af5",
   "metadata": {},
   "source": [
    "### Visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb923db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_airport_dropoff_by_weekday(dataframe_1: pd.DataFrame, dataframe_2: pd.DataFrame,\n",
    "                                    dataframe_3: pd.DataFrame):\n",
    "    \"\"\"Plot the popularity of rides dropping off at airports by weekdays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe1 : pandas.DataFrame\n",
    "        The dataframe containing data with dropoff coordinate within a airport.\n",
    "    dataframe2 : pandas.DataFrame\n",
    "        The dataframe containing data with dropoff coordinate within a airport.\n",
    "    dataframe3 : pandas.DataFrame\n",
    "        The dataframe containing data with dropoff coordinate within a airport.\n",
    "        \n",
    "    \"\"\"\n",
    "    figure, axes = plt.subplots(3, 1, figsize=(20, 10))\n",
    "    \n",
    "    # Plot the bars for each data frame on separate subplots\n",
    "    axes[0].bar(dataframe_1['weekday'], dataframe_1['Count'])\n",
    "    axes[0].set_title('LGA Airport')\n",
    "    axes[0].set_xlabel(\"Weekday\")\n",
    "    axes[0].set_ylabel('Number of Rides')\n",
    "    \n",
    "    axes[1].bar(dataframe_2['weekday'], dataframe_2['Count'])\n",
    "    axes[1].set_title('JFK Airport')\n",
    "    axes[1].set_xlabel(\"Weekday\")\n",
    "    axes[1].set_ylabel('Number of Rides')\n",
    "    \n",
    "    axes[2].bar(dataframe_3['weekday'], dataframe_3['Count'])\n",
    "    axes[2].set_title('EWR Airport')\n",
    "    axes[2].set_xlabel(\"Weekday\")\n",
    "    axes[2].set_ylabel('Number of Rides')\n",
    "    \n",
    "    figure.suptitle('Popularity of rides for each day of the week', fontsize=16)\n",
    "    \n",
    "    # Adjust the subplots spacing and padding\n",
    "    figure.subplots_adjust(hspace=0.4, top=0.9, bottom=0.1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airport_dropoff_by_weekday(coords: tuple) -> pd.DataFrame:\n",
    "    \"\"\"Obtain the popularity of rides dropping off at a airport by weekdays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coords : pandas.DataFrame\n",
    "        The coordinates of the airport.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe including weekdays and corresponding rides that dropped off at the given airport.\n",
    "        \n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        WITH all_trips AS (SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM taxi_trips\n",
    "                           UNION ALL\n",
    "                           SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM uber_trips)\n",
    "        SELECT\n",
    "            CASE\n",
    "                WHEN strftime('%w', pickup_datetime) = '0' THEN 'Sunday'\n",
    "                WHEN strftime('%w', pickup_datetime) = '1' THEN 'Monday'\n",
    "                WHEN strftime('%w', pickup_datetime) = '2' THEN 'Tuesday'\n",
    "                WHEN strftime('%w', pickup_datetime) = '3' THEN 'Wednesday'\n",
    "                WHEN strftime('%w', pickup_datetime) = '4' THEN 'Thursday'\n",
    "                WHEN strftime('%w', pickup_datetime) = '5' THEN 'Friday'\n",
    "                WHEN strftime('%w', pickup_datetime) = '6' THEN 'Saturday'\n",
    "            END AS weekday,\n",
    "            COUNT(*) AS Count\n",
    "        FROM\n",
    "            all_trips\n",
    "        WHERE\n",
    "            dropoff_longitude BETWEEN {coords[0][1]} AND {coords[1][1]}\n",
    "            AND\n",
    "            dropoff_latitude BETWEEN {coords[0][0]} AND {coords[1][0]}\n",
    "        GROUP BY\n",
    "            weekday\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_sql_query(query, engine)\n",
    "    \n",
    "    # Define the order of weekdays as a list\n",
    "    order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    # Convert the \"weekday\" column to a categorical data type with the custom order\n",
    "    dataframe['weekday'] = pd.Categorical(dataframe['weekday'], categories=order, ordered=True)\n",
    "    # Sort the dataframe by the \"weekday\" column\n",
    "    dataframe = dataframe.sort_values('weekday')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_LGA = get_airport_dropoff_by_weekday(LGA_BOX_COORDS)\n",
    "df_2_JFK = get_airport_dropoff_by_weekday(JFK_BOX_COORDS)\n",
    "df_2_EWR = get_airport_dropoff_by_weekday(EWR_BOX_COORDS)\n",
    "plot_airport_dropoff_by_weekday(df_2_LGA, df_2_JFK, df_2_EWR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5c0e3",
   "metadata": {},
   "source": [
    "### Visualization 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(dataframe: pd.DataFrame) -> List[Tuple[float, float]]:\n",
    "    \"\"\"Extract pickup and dropoff coordinates from the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        DataFrame containing pickup and dropoff latitude and longitude data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of pickup and dropoff coordinates as tuples (latitude, longitude).\n",
    "    \"\"\"\n",
    "    pickup_coords = dataframe[['pickup_latitude', 'pickup_longitude']].values.tolist()\n",
    "    dropoff_coords = dataframe[['dropoff_latitude', 'dropoff_longitude']].values.tolist()\n",
    "    return pickup_coords + dropoff_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_pickup_dropoff_locations(dataframe: pd.DataFrame) -> folium.Map:\n",
    "    \"\"\"Create a heatmap of pickup and dropoff locations based on the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        DataFrame containing pickup and dropoff latitude and longitude data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    folium.Map\n",
    "        A folium map object containing the heatmap.\n",
    "    \"\"\"\n",
    "    # Create a base map using the mean latitude and longitude values from the input DataFrame.\n",
    "    m = folium.Map(location=[(dataframe['pickup_latitude'].mean() + dataframe['dropoff_latitude'].mean()) / 2,\n",
    "                              (dataframe['pickup_longitude'].mean() + dataframe['pickup_longitude'].mean()) / 2],\n",
    "                   zoom_start=10)\n",
    "    \n",
    "    # Get all coordinates of pickup and dropoff points\n",
    "    all_coords = extract_coordinates(dataframe)\n",
    "    \n",
    "    # Set heatmap parameters\n",
    "    heatmap_params = {\n",
    "        'radius': 12,\n",
    "        'max_zoom': 18,\n",
    "        'gradient': {0.2: 'blue', 0.4: 'green', 0.6: 'yellow', 0.8: 'orange', 1.0: 'red'}\n",
    "    }\n",
    "    \n",
    "    # Create and add the heatmap to the map\n",
    "    heatmap = HeatMap(all_coords, **heatmap_params)\n",
    "    heatmap.add_to(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef930f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_4() -> pd.DataFrame:\n",
    "    \"\"\"Retrieve the data for plotting a heatmap of pickup and dropoff locations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing pickup and dropoff latitude and longitude data.\n",
    "    \"\"\"\n",
    "    QUERY = \"\"\"\n",
    "    SELECT\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude\n",
    "    FROM\n",
    "        uber_trips;\n",
    "    \"\"\"\n",
    "    result = engine.execute(QUERY).fetchall()\n",
    "    df = pd.DataFrame(result, columns=['pickup_longitude', 'pickup_latitude', \"dropoff_longitude\", \"dropoff_latitude\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_4()\n",
    "heatmap_pickup_dropoff_locations(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048c571",
   "metadata": {},
   "source": [
    "### Visualization 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab85845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tip_distance_taxi(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot the scatter plot of tip amount versus the distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataframe containing data with the tip amount and the distance of rides.\n",
    "    \n",
    "    \"\"\"\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # create a scatter plot\n",
    "    axes.scatter(x=dataframe['distance'], y=dataframe['tip_amount'])\n",
    "    \n",
    "    # set the x and y axis labels\n",
    "    axes.set_xlabel('Distance')\n",
    "    axes.set_ylabel('Tip Amount')\n",
    "    \n",
    "    # set the plot title\n",
    "    axes.set_title(\"Tip Amount vs. Distance\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tip_distance_taxi_mean(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot the scatter plot of the means of tip amount versus the distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataframe containing data with the tip amount and the distance of rides.\n",
    "    \n",
    "    \"\"\"\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # divide the distance into 100 bins and calculate the mean of tip_amount for each bin\n",
    "    mean_tip_amount = dataframe.groupby(pd.cut(dataframe['distance'], bins=100))['tip_amount'].mean()\n",
    "    bin_edges = pd.cut(dataframe['distance'], bins=100, retbins=True)[1]\n",
    "    distance = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # create a line plot of mean tip_amount vs distance\n",
    "    axes.scatter(distance, mean_tip_amount)\n",
    "    \n",
    "    # set the x and y axis labels\n",
    "    axes.set_xlabel('Distance')\n",
    "    axes.set_ylabel('Mean Tip Amount')\n",
    "    \n",
    "    # set the plot title\n",
    "    axes.set_title(\"Mean Tip Amount vs. Distance\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a55d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tip_distance_taxi_combined(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot the scatter plot of tip amount versus the distance and the mean tip amount versus the distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataframe containing data with the tip amount and the distance of rides.\n",
    "    \n",
    "    \"\"\"\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # create a scatter plot of original data\n",
    "    axes.scatter(x=dataframe['distance'], y=dataframe['tip_amount'], label='Original Data', alpha=0.5, color='dodgerblue')\n",
    "    \n",
    "    # divide the distance into 100 bins and calculate the mean of tip_amount for each bin\n",
    "    mean_tip_amount = dataframe.groupby(pd.cut(dataframe['distance'], bins=100))['tip_amount'].mean()\n",
    "    bin_edges = pd.cut(dataframe['distance'], bins=100, retbins=True)[1]\n",
    "    distance = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # create a secondary y-axis for the mean tip_amount\n",
    "    ax2 = axes.twinx()\n",
    "    \n",
    "    # create a line plot of mean tip_amount vs distance on the secondary y-axis\n",
    "    ax2.scatter(distance, mean_tip_amount, color='darkorange', label='Mean Tip Amount', marker='o')\n",
    "    \n",
    "    # set the x and y axis labels\n",
    "    axes.set_xlabel('Distance')\n",
    "    axes.set_ylabel('Tip Amount')\n",
    "    ax2.set_ylabel('Mean Tip Amount')\n",
    "    \n",
    "    # set the plot title\n",
    "    axes.set_title(\"Tip Amount vs. Distance and Mean Tip Amount vs. Distance\")\n",
    "    \n",
    "    # add legends for both axes\n",
    "    axes.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456bb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tip_distance_taxi() -> pd.DataFrame:\n",
    "    \"\"\"Obtain tip amounts and distances of yellow taxi rides.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including tip amounts and distances of yellow taxi rides.\n",
    "    \n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            tip_amount,\n",
    "            distance\n",
    "        FROM\n",
    "            taxi_trips\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_sql_query(query, engine)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tip_distance = data_tip_distance_taxi()\n",
    "df_tip_distance = remove_outliers_std(df_tip_distance, 'tip_amount')\n",
    "df_tip_distance = remove_outliers_std(df_tip_distance, 'distance')\n",
    "plot_tip_distance_taxi_combined(df_tip_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bc306",
   "metadata": {},
   "source": [
    "### Visualization 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd011d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binned_means(dataframe: pd.DataFrame, x_col: str, y_col: str, num_bins: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the mean values for specified number of bins.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    x_col : str\n",
    "        The x-axis column name.\n",
    "    y_col : str\n",
    "        The y-axis column name.\n",
    "    num_bins : int\n",
    "        The number of bins to be created.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe containing the binned mean values.\n",
    "    \"\"\"\n",
    "    # Calculate the bins and bin the data\n",
    "    bins = np.linspace(dataframe[x_col].min(), dataframe[x_col].max(), num_bins + 1)\n",
    "    dataframe['binned'] = pd.cut(dataframe[x_col], bins)\n",
    "    \n",
    "    # Calculate the mean value for each bin and clean up the output dataframe\n",
    "    binned_means = dataframe.groupby('binned')[y_col].mean().reset_index()\n",
    "    binned_means[x_col] = [(interval.left + interval.right) / 2 for interval in binned_means['binned']]\n",
    "    del binned_means['binned']\n",
    "    \n",
    "    return binned_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b320b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_data(ax: matplotlib.axes.Axes, dataframe: pd.DataFrame, x_col: str, y_col: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot the original data as a scatter plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to plot on.\n",
    "    dataframe : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    x_col : str\n",
    "        The x-axis column name.\n",
    "    y_col : str\n",
    "        The y-axis column name.\n",
    "    \"\"\"\n",
    "    # Plot the original data\n",
    "    ax.scatter(x=dataframe[x_col], y=dataframe[y_col], alpha=0.5, color='#89CFF0', label='Original Data')\n",
    "    ax.set_xlabel(f\"{x_col} (inches to hundredths)\")\n",
    "    ax.set_ylabel(f\"{y_col} (dollars)\")\n",
    "    ax.yaxis.label.set_color('#89CFF0')\n",
    "    ax.tick_params(axis='y', colors='#89CFF0')\n",
    "    ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binned_means(ax: matplotlib.axes.Axes, binned_means: pd.DataFrame, x_col: str, y_col: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot the binned mean values as a scatter plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to plot on.\n",
    "    binned_means : pd.DataFrame\n",
    "        The dataframe containing binned mean values.\n",
    "    x_col : str\n",
    "        The x-axis column name.\n",
    "    y_col : str\n",
    "        The y-axis column name.\n",
    "    \"\"\"\n",
    "    # Plot the binned means\n",
    "    ax.scatter(x=binned_means[x_col], y=binned_means[y_col], color='#FFA07A', label='Binned Means')\n",
    "    ax.set_ylabel('Binned Means(dollars)')\n",
    "    ax.yaxis.label.set_color('#FFA07A')\n",
    "    ax.tick_params(axis='y', colors='#FFA07A')\n",
    "    ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Understand the relationship between hourly precipitation and tip amount, we should not only use the raw data but also use the binned data\n",
    "# The raw data are not enough for us to get a reasonable conclusion\n",
    "def plot_scatter_with_original_data_and_binned_means(dataframe: pd.DataFrame, x_col: str = \"hourly_precipitation\", \n",
    "                                                     y_col: str = \"tip\", num_bins: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    Plot the original data and binned mean values on the same plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    x_col : str, optional\n",
    "        The x-axis column name. Defaults to \"hourly_precipitation\".\n",
    "    y_col : str, optional\n",
    "        The y-axis column name. Defaults to \"tip\".\n",
    "    num_bins : int, optional\n",
    "        The number of bins to be created. Defaults to 100.\n",
    "    \"\"\"\n",
    "    binned_means = calculate_binned_means(dataframe, x_col, y_col, num_bins)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    plot_original_data(ax1, dataframe, x_col, y_col)\n",
    "    ax2 = ax1.twinx()\n",
    "    plot_binned_means(ax2, binned_means, x_col, y_col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_6() -> pd.DataFrame:\n",
    "    \"\"\"Retrieve and preprocess data for Visual 6.\n",
    "    \n",
    "    This function queries hourly weather and taxi trips data, merges the two dataframes on the 'time' column, \n",
    "    and removes outliers from the 'hourly_precipitation' and 'tip' columns using the standard deviation method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A preprocessed dataframe with columns 'hourly_precipitation' and 'tip' without outliers.\n",
    "    \"\"\"\n",
    "    QUERY1 =\"\"\" \n",
    "    SELECT strftime('%Y-%m-%d %H'||\":00:00\", h.date) AS time, h.hourly_precipitation AS hourly_precipitation\n",
    "    FROM hourly_weather h\n",
    "    \"\"\"\n",
    "    QUERY2 =\"\"\"\n",
    "    SELECT strftime('%Y-%m-%d %H'||\":00:00\", t.pickup_datetime) AS time, t.tip_amount AS tip\n",
    "    FROM taxi_trips t\n",
    "    \"\"\"\n",
    "    hourly_weather_data = pd.read_sql_query(QUERY1, engine)\n",
    "    taxi_trips_data = pd.read_sql_query(QUERY2, engine)\n",
    "    \n",
    "    df = pd.merge(hourly_weather_data,taxi_trips_data,on=\"time\" )\n",
    "    df = df.drop(\"time\",axis=1)\n",
    "    # Delete the outliers\n",
    "    df_no_outliers = remove_outliers_std(df, 'hourly_precipitation')\n",
    "    df_no_outliers = remove_outliers_std(df_no_outliers, 'tip')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_6()\n",
    "plot_scatter_with_original_data_and_binned_means(some_dataframe)\n",
    "# We can observe that the binned means of tips remain relatively stable when the hourly precipitation is below 0.2. \n",
    "# As the hourly precipitation increases, the binned means of tips fluctuate more dramatically. \n",
    "# When the hourly precipitation exceeds 1.2, the binned means of tips approach zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321010c",
   "metadata": {},
   "source": [
    "# Extra Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea129e5",
   "metadata": {},
   "source": [
    "### Visualization: The relationship between daytime duration and average travel distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8554d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema for the \"sunrise and sunset table\"\n",
    "SUNRISE_AND_SUNSET_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sunrise_and_sunset (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATETIME,\n",
    "    sunrise DATETIME,\n",
    "    sunset DATETIME\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a96d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(SUNRISE_AND_SUNSET_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sunrise_and_sunset_data from daily_weather_data\n",
    "sunrise_and_sunset_data = daily_weather_data[[\"date\", \"sunrise\", \"sunset\"]]\n",
    "# Drop the useless rows\n",
    "sunrise_and_sunset_data = sunrise_and_sunset_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionry for writting the data into the table\n",
    "map_table_name_to_dataframe_for_extra_credits_question = {\n",
    "    \"sunrise_and_sunset\": sunrise_and_sunset_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d277dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write table into the new table\n",
    "write_dataframes_to_table(map_table_name_to_dataframe_for_extra_credits_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_data_for_this_visualization_problem() -> pd.DataFrame:\n",
    "    \"\"\"Fetch data for the visualization problem.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe containing date, daytime duration, and average distance.\n",
    "    \"\"\"\n",
    "    Query = \"\"\"\n",
    "    WITH daily_weather_duration AS (\n",
    "        SELECT date, \n",
    "               CAST((julianday(sunset) - julianday(sunrise)) * 24 * 60 AS REAL) AS daytime_duration\n",
    "        FROM sunrise_and_sunset\n",
    "    ),\n",
    "\n",
    "    combined_trips AS (\n",
    "        SELECT pickup_datetime, distance, date(pickup_datetime) AS date\n",
    "        FROM taxi_trips\n",
    "        WHERE pickup_datetime IS NOT NULL AND distance IS NOT NULL\n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime, distance, date(pickup_datetime) AS date\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime IS NOT NULL AND distance IS NOT NULL\n",
    "    ),\n",
    "\n",
    "    trips_with_daytime_duration AS (\n",
    "        SELECT c.pickup_datetime, c.distance, c.date, d.daytime_duration\n",
    "        FROM combined_trips c\n",
    "        JOIN daily_weather_duration d ON c.date = d.date\n",
    "    )\n",
    "\n",
    "    SELECT date, \n",
    "           AVG(daytime_duration) AS daytime_duration_minutes,\n",
    "           AVG(distance) AS avg_distance\n",
    "    FROM trips_with_daytime_duration\n",
    "    GROUP BY date\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(Query, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fe05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binned_means(df: pd.DataFrame, x_col: str, y_col: str, num_bins: int) -> pd.DataFrame:\n",
    "    \"\"\"Calculate binned means for the specified columns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input dataframe.\n",
    "    x_col : str\n",
    "        The name of the x-axis column.\n",
    "    y_col : str\n",
    "        The name of the y-axis column.\n",
    "    num_bins : int\n",
    "        The number of bins to use.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe containing the binned means.\n",
    "    \"\"\"\n",
    "    bin_edges = pd.cut(df[x_col], bins=num_bins, retbins=True)[1]\n",
    "    binned_means = df.groupby(pd.cut(df[x_col], bins=bin_edges)).mean()\n",
    "    binned_means = binned_means[[x_col, y_col]].reset_index(drop=True)\n",
    "    return binned_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daytime_duration_vs_avg_distance(dataframe: pd.DataFrame, x_col: str = \"daytime_duration_minutes\", \n",
    "                                          y_col: str = \"avg_distance\", num_bins: int = 20) -> None:\n",
    "    \"\"\"Plot daytime duration vs average distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        The input dataframe.\n",
    "    x_col : str, optional\n",
    "        The name of the x-axis column, by default \"daytime_duration_minutes\".\n",
    "    y_col : str, optional\n",
    "        The name of the y-axis column, by default \"avg_distance\".\n",
    "    num_bins : int, optional\n",
    "        The number of bins to use, by default 20.\n",
    "    \"\"\"\n",
    "    df_no_outliers = remove_outliers_std(dataframe, x_col)\n",
    "    df_no_outliers = remove_outliers_std(df_no_outliers, y_col)\n",
    "    \n",
    "    binned_means = calculate_binned_means(df_no_outliers, x_col, y_col, num_bins)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plot original data\n",
    "    ax1.scatter(x=df_no_outliers[x_col], y=df_no_outliers[y_col], alpha=0.5, color='#89CFF0', label='Original Data')\n",
    "    ax1.set_xlabel(f\"{x_col} (minutes)\")\n",
    "    ax1.set_ylabel(f\"{y_col} (miles)\")\n",
    "    ax1.yaxis.label.set_color('#89CFF0')\n",
    "    ax1.tick_params(axis='y', colors='#89CFF0')\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Plot binned means\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(x=binned_means[x_col], y=binned_means[y_col], color='#FFA07A', label='Binned Means')\n",
    "    ax2.set_ylabel('Binned Means (miles)')\n",
    "    ax2.yaxis.label.set_color('#FFA07A')\n",
    "    ax2.tick_params(axis='y', colors='#FFA07A')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.title(\"Daytime Duration vs Average Distance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e10b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = get_the_data_for_this_visualization_problem()\n",
    "plot_daytime_duration_vs_avg_distance(dataframe)\n",
    "# We can see that when daytime duration increases, the average distance of trips goes up too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f2746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
