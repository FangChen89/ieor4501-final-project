{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a622c55a",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "## IEOR E4501 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import distance\n",
    "import requests\n",
    "import re\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "TAXI_DIR = \"data/taxi\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"data/uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"data/weather\"\n",
    "\n",
    "CRS = 4326 # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "    print(f\"Folder {QUERY_DIRECTORY} created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder {QUERY_DIRECTORY} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the TAXI_DIR exists\n",
    "try:\n",
    "    os.mkdir(TAXI_DIR)\n",
    "    print(f\"Folder {TAXI_DIR} created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder {TAXI_DIR} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the shapefile and return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shapefile : str\n",
    "        The relative path of the shape file including zone IDs and geometries.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "    \n",
    "    \"\"\"\n",
    "    loaded_taxi_zones = gpd.read_file(shapefile)\n",
    "    loaded_taxi_zones = loaded_taxi_zones[['OBJECTID', 'geometry']].set_index('OBJECTID')\n",
    "    # Transform geometries to the new coordinate reference system 4326\n",
    "    loaded_taxi_zones = loaded_taxi_zones.to_crs(CRS)\n",
    "    \n",
    "    return loaded_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id: int,\n",
    "                                   loaded_taxi_zones: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Given the zone ID and return the corresponding centroid coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zone_loc_id : int\n",
    "        The zone ID which needs to be searched.\n",
    "    loaded_taxi_zones : pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "\n",
    "    \"\"\"\n",
    "    geometry = loaded_taxi_zones.loc[zone_loc_id, 'geometry']\n",
    "    # Obtain the approximate coordinates by the centroid location\n",
    "    longitude = geometry.centroid.x\n",
    "    latitude = geometry.centroid.y\n",
    "    coords = (longitude, latitude)\n",
    "    \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord: tuple, to_coord: tuple) -> float:\n",
    "    \"\"\"Given the coordinates and return the distance between them.\n",
    "    \n",
    "    This function utilizes the Haversine formula to calculate the distance\n",
    "    between two coordinates on Earth's surface.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    from_coord : tuple of float\n",
    "        A tuple containing the longitude and latitude of the starting point, expressed in degrees.\n",
    "    to_coord : tuple of float\n",
    "        A tuple containing the longitude and latitude of the destination point, expressed in degrees.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The distance between the two coordinates, in kilometers.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input coordinates from degrees to radians\n",
    "    from_lon, from_lat = math.radians(from_coord[0]), math.radians(from_coord[1])\n",
    "    to_lon, to_lat = math.radians(to_coord[0]), math.radians(to_coord[1])\n",
    "    # Calculate the differences in latitude and longitude\n",
    "    delta_lon = to_lon - from_lon\n",
    "    delta_lat = to_lat - from_lat\n",
    "    # Apply the Haversine formula to calculate the distance\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(from_lat) * math.cos(to_lat) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = 6371 * c # earth's radius is assumed to be 6371 kilometers\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add the 'distance' column to the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the new calculated column 'distance'.\n",
    "    \n",
    "    \"\"\"\n",
    "    dataframe['distance'] = dataframe.apply(lambda x:calculate_distance_with_coords(\n",
    "                                (x['pickup_longitude'],x['pickup_latitude']),\n",
    "                                (x['dropoff_longitude'],x['dropoff_latitude'])), axis=1)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c0aeb",
   "metadata": {},
   "source": [
    "### Remove outside trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outside_trip(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove the trip records outside the defined region.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after removing all trip records outside the defined region.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Obtain the coordinate limits\n",
    "    southlimit, westlimit = NEW_YORK_BOX_COORDS[0]\n",
    "    northlimit, eastlimit = NEW_YORK_BOX_COORDS[1]\n",
    "    # Remove the trips outside the location \n",
    "    df = df[(df['pickup_longitude'] >= westlimit) & (df['pickup_longitude'] <= eastlimit)]\n",
    "    df = df[(df['pickup_latitude'] >= southlimit) & (df['pickup_latitude'] <= northlimit)]\n",
    "\n",
    "    df = df[(df['dropoff_longitude'] >= westlimit) & (df['dropoff_longitude'] <= eastlimit)]\n",
    "    df = df[(df['dropoff_latitude'] >= southlimit) & (df['dropoff_latitude'] <= northlimit)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page: str) -> list:\n",
    "    \"\"\"Scrap the URLs from the given page and return them as a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    taxi_page : str\n",
    "        The URL of the target page.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of URLs scraped from the given page.\n",
    "        \n",
    "    \"\"\"\n",
    "    all_urls = list()\n",
    "    \n",
    "    content = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(content.text, 'lxml')\n",
    "    # Find all the URLs in the page\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        all_urls.append(link.get('href'))\n",
    "        \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_parquet_urls(all_urls: list) -> list:\n",
    "    \"\"\"Find the URLs for yellow taxi data and return them as a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_urls : str\n",
    "        A list of URLs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of URLs for yellow taxi parquet files from 2019-01 to 2015-06.\n",
    "        \n",
    "    \"\"\"\n",
    "    all_parquet_urls = list()\n",
    "    \n",
    "    pattern = r\".*yellow_tripdata.*parquet\\Z\"\n",
    "    time_pattern = r\"(2009-(0[1-9]|1[0-2]))|(201[0-4]-(0[1-9]|1[0-2]))|(2015-(0[1-6]))\"\n",
    "    for url in all_urls:\n",
    "        # Check if the URL belongs to yellow taxi trip data\n",
    "        if re.search(pattern, url):\n",
    "            # Check if the URL belongs to the time range for the project\n",
    "            if re.search(time_pattern, url):\n",
    "                all_parquet_urls.append(url)\n",
    "            \n",
    "    return all_parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coords_from_zones(dataframe):\n",
    "    \"\"\"Generate the coordinates from zone IDs in a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after transforming zone IDs to longitude and latitude coordinates.\n",
    "    \n",
    "    \"\"\"\n",
    "    loaded_taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    southlimit, westlimit = NEW_YORK_BOX_COORDS[0]\n",
    "    northlimit, eastlimit = NEW_YORK_BOX_COORDS[1]\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        pickup_zoneid = row['pickup_zoneid']\n",
    "        dropoff_zoneid = row['dropoff_zoneid']\n",
    "\n",
    "        pickup_coords = lookup_coords_for_taxi_zone_id(pickup_zoneid, loaded_taxi_zones)\n",
    "        \n",
    "        # define the initial bearing\n",
    "        direction = 0\n",
    "        # check if pickup and dropoff zones are the same\n",
    "        if pickup_zoneid == dropoff_zoneid:\n",
    "            # generate dropoff coordinates using distance and bearing\n",
    "            dropoff_coords = distance(\n",
    "                miles=row['trip_distance']).destination(pickup_coords[::-1], bearing=direction)[1::-1]\n",
    "        else:\n",
    "            # generate dropoff coordinates using dropoff zone ID\n",
    "            dropoff_coords = lookup_coords_for_taxi_zone_id(dropoff_zoneid, loaded_taxi_zones)\n",
    "        \n",
    "        # check if dropoff coordinates fall outside the defined box\n",
    "        while not ((westlimit <= dropoff_coords[0] <= eastlimit) and \n",
    "                   (southlimit <= dropoff_coords[1] <= northlimit)):\n",
    "            # Generate new dropoff coordinates by changing the bearing\n",
    "            direction += 90\n",
    "            # If all four bearings do not work, drop this record instead\n",
    "            if direction == 360:\n",
    "                break\n",
    "            dropoff_coords = distance(\n",
    "                miles=row['trip_distance']).destination(pickup_coords, bearing=direction)[1::-1]\n",
    "        \n",
    "        # update the dataframe with the generated coordinates\n",
    "        if direction != 360:\n",
    "            dataframe.loc[index, 'pickup_longitude'] = pickup_coords[0]\n",
    "            dataframe.loc[index, 'pickup_latitude'] = pickup_coords[1]\n",
    "            dataframe.loc[index, 'dropoff_longitude'] = dropoff_coords[0]\n",
    "            dataframe.loc[index, 'dropoff_latitude'] = dropoff_coords[1]\n",
    "        else:\n",
    "            dataframe.drop(index=index, inplace=True)\n",
    "    \n",
    "    # Drop the unnecessary columns\n",
    "    dataframe.drop(['trip_distance', 'pickup_zoneid', 'dropoff_zoneid'], axis=1, inplace=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_df_2009_to_2010(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the yellow taxi data from 2009 to 2010.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe with records from 2009 to 2010.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after normalizing column names, removing invalid data, and sampling.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Normalize the column names\n",
    "    dataframe.columns = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "                         'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                         'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', \n",
    "                         'payment_type', 'fare_amount', 'surcharge', 'mta_tax', 'tip_amount', \n",
    "                         'tolls_amount', 'total_amount']\n",
    "    \n",
    "    # Remove the trips outside the required coordinate box\n",
    "    dataframe = remove_outside_trip(dataframe)\n",
    "    # Remove the trips with zero passenger count\n",
    "    dataframe = dataframe[dataframe['passenger_count'] != 0]\n",
    "    # Remove the trips without a fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] != 0]\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[dataframe['trip_distance'] != 0]\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] != dataframe['dropoff_longitude']) \n",
    "                          & (dataframe['pickup_latitude'] != dataframe['dropoff_latitude'])]\n",
    "    \n",
    "    # Sample the taxi data at a appropriate number\n",
    "    dataframe = dataframe.sample(n = 2500, random_state=1)\n",
    "    \n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "                       'dropoff_longitude', 'dropoff_latitude', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "    \n",
    "    # Transform the pickup datetime column from strings to datetime\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'],\n",
    "                                                  format='%Y-%m-%d %H:%M:%S') \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f900c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_df_2011_to_2015(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the yellow taxi data from 2011 to 2015.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe with records from 2011 to 2015.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after normalizing column names, removing invalid data, and sampling.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Normalize the column names\n",
    "    dataframe.columns = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "                         'trip_distance', 'rate_code', 'store_and_fwd_flag', 'pickup_zoneid',\n",
    "                         'dropoff_zoneid', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax',\n",
    "                         'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "                         'congestion_surcharge', 'airport_fee']\n",
    "    \n",
    "    # Remove the trips outside the 1-263 zones\n",
    "    dataframe = dataframe[dataframe['pickup_zoneid'] != 264]\n",
    "    dataframe = dataframe[dataframe['pickup_zoneid'] != 265]\n",
    "    dataframe = dataframe[dataframe['dropoff_zoneid'] != 264]\n",
    "    dataframe = dataframe[dataframe['dropoff_zoneid'] != 265]\n",
    "    # Remove the trips with zero passenger count\n",
    "    dataframe = dataframe[dataframe['passenger_count'] != 0]\n",
    "    # Remove the trips without a fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] != 0]\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[dataframe['trip_distance'] != 0]\n",
    "    \n",
    "    # Sample the taxi data at a appropriate number\n",
    "    dataframe = dataframe.sample(n = 2500, random_state=1)\n",
    "    \n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['pickup_datetime', 'trip_distance',\n",
    "                       'pickup_zoneid', 'dropoff_zoneid', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "    \n",
    "    # Generate the coordinates from zone IDs\n",
    "    dataframe = generate_coords_from_zones(dataframe)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean the parquet file for the URL, return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL for the parquet file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after loading and cleaning the parquet file from the given URL.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Programmatically download needed data if not exists\n",
    "    dataframe = pd.DataFrame()\n",
    "    \n",
    "    time_pattern = r\"(2009-(0[1-9]|1[0-2]))|(201[0-4]-(0[1-9]|1[0-2]))|(2015-(0[1-6]))\"\n",
    "    time = \"\"\n",
    "    \n",
    "    if re.search(time_pattern, url):\n",
    "        time = re.search(time_pattern, url).group(0)\n",
    "        file_path = f\"{TAXI_DIR}/yellow_taxi_{time}.parquet\"\n",
    "        \n",
    "        # Check if the parquet file has already been downloaded\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File {file_path} already exists.\")\n",
    "            dataframe = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        else:\n",
    "            # If not, download the file from the given URL\n",
    "            print(f\"File {file_path} does not exist. Downloading...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                print(f\"File {file_path} downloaded successfully.\")\n",
    "            dataframe = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    \n",
    "    # Clean the dataframe\n",
    "    if re.search(r\"2009|2010\", time):\n",
    "        dataframe_cleaned = clean_taxi_df_2009_to_2010(dataframe)\n",
    "    else:\n",
    "        dataframe_cleaned = clean_taxi_df_2011_to_2015(dataframe)\n",
    "    \n",
    "    return dataframe_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls: list) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess and concatenate all the data, return them as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parquet_urls : list\n",
    "        A list of URLs for parquet files of yellow taxi data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after preprocessing and concatenating all the parquet file data.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    # Iterate the URLs and obtain the dataframe for each month\n",
    "    for parquet_url in parquet_urls:\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # Add the 'distance' column\n",
    "        dataframe = add_distance_column(dataframe)\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "\n",
    "    # Create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() -> pd.DataFrame:\n",
    "    \"\"\"Scrap the yellow taxi data and return the result as a DataFrame.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including all cleaned and sampled records for yellow taxi data.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sampled yellow taxi data as a CSV file\n",
    "taxi_data.to_csv('data/taxi/yellow_taxi_sampled.csv', index=False)\n",
    "# Load the sampled yellow taxi data directly\n",
    "taxi_data = pd.read_csv('data/taxi/yellow_taxi_sampled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean the Uber data and return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_file : str\n",
    "        The relative path of the CSV file of Uber data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after loading and cleaning all the Uber data.\n",
    "    \n",
    "    \"\"\"\n",
    "    columns_to_keep = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "                       'dropoff_longitude', 'dropoff_latitude']\n",
    "    dataframe = pd.read_csv(csv_file, usecols = columns_to_keep)\n",
    "    # Transform the pickup datetime column from strings to datetime\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'],\n",
    "                                                  format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    dataframe['pickup_datetime'] = dataframe['pickup_datetime'].dt.tz_convert(None)\n",
    "    \n",
    "    # Remove the trips outside the defined coordinate box\n",
    "    dataframe = remove_outside_trip(dataframe)\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] != dataframe['dropoff_longitude']) \n",
    "                          & (dataframe['pickup_latitude'] != dataframe['dropoff_latitude'])]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame:\n",
    "    \"\"\"Return the processed Uber data as a DataFrame.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after preprocessing all the Uber data and adding column 'distance'.\n",
    "    \n",
    "    \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    # Add the 'distance' column\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory: str) -> list:\n",
    "    \"\"\"Load and clean the CSV files, return them as a list of DataFrame.\"\"\"\n",
    "    # List all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    # Initialize an empty list to store dataframes\n",
    "    dfs = []\n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed',\n",
    "                       'DailyAverageWindSpeed', 'DailySustainedWindSpeed',\n",
    "                       'Sunrise', 'Sunset']\n",
    "    \n",
    "    # Iterate through the CSV files, read each file as a dataframe, and append it to the list\n",
    "    for csv_file in csv_files:\n",
    "        # Load the CSV file as a DataFrame\n",
    "        file_path = os.path.join(directory, csv_file)\n",
    "        df = pd.read_csv(file_path, usecols=columns_to_keep, engine='python')\n",
    "        \n",
    "        # Transform the 'DATE' column from strings to datetime\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'], format='%Y-%m-%dT%H:%M:%S')\n",
    "        \n",
    "        # Fill in the missing daily records\n",
    "        # Create a date range for each year with daily frequency\n",
    "        min_date = df['DATE'].min().normalize()\n",
    "        max_date = df['DATE'].max().normalize()\n",
    "        date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "        # Create a list to hold the missing daily records\n",
    "        missing_dates = []\n",
    "        # Iterate through the date range and check if there is a missing record\n",
    "        for date in date_range:\n",
    "            date_23_59 = date.replace(hour=23, minute=59, second=0)\n",
    "            if not ((df['DATE'] == date_23_59).any()):\n",
    "                missing_date_row = {'DATE': date_23_59}\n",
    "                missing_dates.append(missing_date_row)\n",
    "        # Create a dataframe from the missing daily records\n",
    "        missing_df = pd.DataFrame(missing_dates)\n",
    "        # Append the generated dataframe to the original dataframe\n",
    "        new_df = pd.concat([df, missing_df], ignore_index=True)\n",
    "        # Sort the new dataframe by the column 'DATE'\n",
    "        new_df.sort_values(by='DATE', inplace=True)\n",
    "        \n",
    "        # Iterate through the dataframe and fill in the missing measurement if any\n",
    "        for index, row in new_df.iterrows():\n",
    "            # Check if the column 'DailyAverageWindSpeed' for daily records is missing\n",
    "            if row['DATE'].strftime('%H:%M') == '23:59' and pd.isna(row['DailyAverageWindSpeed']):\n",
    "                date = row['DATE'].date()\n",
    "                # Calculate the mean hourly wind speed for the current date\n",
    "                hourly_wind_speed_mean = new_df.loc[(new_df['DATE'].dt.date == date) \n",
    "                                                    & (new_df['DATE'] != row['DATE']) \n",
    "                                                    & (~new_df['HourlyWindSpeed'].isna()), \n",
    "                                                    'HourlyWindSpeed'].mean()\n",
    "                # Replace the NaN value with the calculated mean value\n",
    "                new_df.loc[(new_df['DATE'].dt.date == date) \n",
    "                           & (new_df['DATE'] == row['DATE']) \n",
    "                           & (new_df['DailyAverageWindSpeed'].isna()), \n",
    "                           'DailyAverageWindSpeed'] = hourly_wind_speed_mean\n",
    "            \n",
    "            # Check if the column 'DailySustainedWindSpeed' for daily records is missing\n",
    "            if row['DATE'].strftime('%H:%M') == '23:59' and pd.isna(row['DailySustainedWindSpeed']):\n",
    "                date = row['DATE'].date()\n",
    "                # Calculate the maximum hourly wind speed for the current date\n",
    "                hourly_wind_speed_max = new_df.loc[(new_df['DATE'].dt.date == date)\n",
    "                                                   & (new_df['DATE'] != row['DATE'])\n",
    "                                                   & (~new_df['HourlyWindSpeed'].isna()),\n",
    "                                                   'HourlyWindSpeed'].max()\n",
    "                # Replace the NaN value with the calculated maximum value\n",
    "                new_df.loc[(new_df['DATE'].dt.date == date) \n",
    "                           & (new_df['DATE'] == row['DATE']) \n",
    "                           & (new_df['DailySustainedWindSpeed'].isna()), \n",
    "                           'DailySustainedWindSpeed'] = hourly_wind_speed_max\n",
    "        \n",
    "        # Append the processed dataframe to the list\n",
    "        dfs.append(new_df)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the DataFrame and return only hourly weather data.\"\"\"\n",
    "    # Split the DataFrame into daily data and hourly data\n",
    "    daily_data = csv_file[(csv_file['DATE'].dt.hour == 23) & (csv_file['DATE'].dt.minute == 59)]\n",
    "    hourly_data = csv_file.drop(daily_data.index)\n",
    "\n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']\n",
    "    hourly_data = hourly_data[columns_to_keep]\n",
    "\n",
    "    # Clean the 'HourlyPrecipitation' data\n",
    "    hourly_data['HourlyPrecipitation'] = hourly_data['HourlyPrecipitation'].str.replace('s', '')\n",
    "    hourly_data['HourlyPrecipitation'] = hourly_data['HourlyPrecipitation'].replace('T', 0)\n",
    "    hourly_data['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    # Transform the 'HourlyPrecipitation' column to float type\n",
    "    hourly_data['HourlyPrecipitation'] = pd.to_numeric(hourly_data['HourlyPrecipitation'])\n",
    "\n",
    "    # Clean the 'HourlyWindSpeed' data\n",
    "    hourly_data['HourlyWindSpeed'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Normalize the column names\n",
    "    hourly_data.columns = ['date', 'hourly_precipitation', 'hourly_windspeed']\n",
    "    \n",
    "    # Return the hourly data\n",
    "    return hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_time_string(value: float) -> str:\n",
    "    \"\"\"Transform the float value to time string in HH:MM format.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    else:\n",
    "        hours = int(value // 100)\n",
    "        minutes = int(value % 100)\n",
    "    return f\"{hours:02d}:{minutes:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the DataFrame and return only daily weather data.\"\"\"\n",
    "    # Split the DataFrame into daily data and hourly data\n",
    "    daily_data = csv_file[(csv_file['DATE'].dt.hour == 23) & (csv_file['DATE'].dt.minute == 59)]\n",
    "    hourly_data = csv_file.drop(daily_data.index)\n",
    "\n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['DATE', 'DailyAverageWindSpeed', 'DailySustainedWindSpeed', 'Sunrise', 'Sunset']\n",
    "    daily_data = daily_data[columns_to_keep]\n",
    "\n",
    "    # Clean the 'Sunrise' and 'Sunset' data\n",
    "    # Convert 'Sunrise' and 'Sunset' columns to strings in HH:MM format\n",
    "    daily_data['Sunrise'] = daily_data['Sunrise'].apply(float_to_time_string)\n",
    "    daily_data['Sunset'] = daily_data['Sunset'].apply(float_to_time_string)\n",
    "    # Convert 'Sunrise' and 'Sunset' columns to time\n",
    "    daily_data['Sunrise'] = pd.to_datetime(daily_data['Sunrise'], format=\"%H:%M\").dt.time\n",
    "    daily_data['Sunset'] = pd.to_datetime(daily_data['Sunset'], format=\"%H:%M\").dt.time\n",
    "    \n",
    "    # Normalize the column names\n",
    "    daily_data.columns = ['date', 'daily_average_windspeed', 'daily_sustained_windspeed',\n",
    "                          'sunrise', 'sunset']\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> tuple:\n",
    "    \"\"\"Load and clean weather data, return hourly/daily records as DataFrames.\"\"\"\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table schema\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_windspeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATETIME,\n",
    "    daily_average_windspeed FLOAT,\n",
    "    daily_sustained_windspeed FLOAT,\n",
    "    sunrise TIME,\n",
    "    sunset TIME\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict: dict) -> None:\n",
    "    for table, dataframe in table_to_df_dict.items():\n",
    "        dataframe.to_sql(table, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
