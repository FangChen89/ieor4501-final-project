{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a622c55a",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import distance\n",
    "import requests\n",
    "import re\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "TAXI_DIR = \"data/taxi\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"data/uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "    print(f\"Folder {QUERY_DIRECTORY} created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder {QUERY_DIRECTORY} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the TAXI_DIR exists\n",
    "try:\n",
    "    os.mkdir(TAXI_DIR)\n",
    "    print(f\"Folder {TAXI_DIR} created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder {TAXI_DIR} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the shapefile and return it as a DataFrame.\"\"\"\n",
    "    loaded_taxi_zones = gpd.read_file(shapefile)\n",
    "    loaded_taxi_zones = loaded_taxi_zones[['OBJECTID', 'geometry']].set_index('OBJECTID')\n",
    "    # Transform geometries to the new coordinate reference system 4326\n",
    "    loaded_taxi_zones = loaded_taxi_zones.to_crs(CRS)\n",
    "    \n",
    "    return loaded_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id: int,\n",
    "                                   loaded_taxi_zones: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Given the zone ID and return the corresponding centroid coordinates.\"\"\"\n",
    "    geometry = loaded_taxi_zones.loc[zone_loc_id, 'geometry']\n",
    "    # Obtain the approximate coordinates by the centroid location\n",
    "    longitude = geometry.centroid.x\n",
    "    latitude = geometry.centroid.y\n",
    "    coords = (longitude, latitude)\n",
    "    \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord: tuple, to_coord: tuple) -> float:\n",
    "    \"\"\"Given the coordinates and return the distance between them.\n",
    "    \n",
    "    This function utilizes the Haversine formula to calculate the distance\n",
    "    between two coordinates on Earth's surface.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    from_coord : tuple of float\n",
    "        A tuple containing the longitude and latitude of the starting point, expressed in degrees.\n",
    "    to_coord : tuple of float\n",
    "        A tuple containing the longitude and latitude of the destination point, expressed in degrees.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The distance between the two coordinates, in kilometers.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input coordinates from degrees to radians\n",
    "    from_lon, from_lat = math.radians(from_coord[0]), math.radians(from_coord[1])\n",
    "    to_lon, to_lat = math.radians(to_coord[0]), math.radians(to_coord[1])\n",
    "    # Calculate the differences in latitude and longitude\n",
    "    delta_lon = to_lon - from_lon\n",
    "    delta_lat = to_lat - from_lat\n",
    "    # Apply the Haversine formula to calculate the distance\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(from_lat) * math.cos(to_lat) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = 6371 * c # earth's radius is assumed to be 6371 kilometers\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add the 'distance' column to the dataframe.\"\"\"\n",
    "    dataframe['distance'] = dataframe.apply(lambda x:calculate_distance_with_coords(\n",
    "                                (x['pickup_longitude'],x['pickup_latitude']),\n",
    "                                (x['dropoff_longitude'],x['dropoff_latitude'])), axis=1)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page: str) -> list:\n",
    "    \"\"\"Scrap the URLs from the given page and return them as a list.\"\"\"\n",
    "    all_urls = list()\n",
    "    \n",
    "    content = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(content.text, 'lxml')\n",
    "    # Find all the URLs in the page\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        all_urls.append(link.get('href'))\n",
    "        \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_parquet_urls(all_urls: list) -> list:\n",
    "    \"\"\"Find the URLs for yellow taxi data and return them as a list.\"\"\"\n",
    "    all_parquet_urls = list()\n",
    "    \n",
    "    pattern = r\".*yellow_tripdata.*parquet\\Z\"\n",
    "    time_pattern = r\"(2009-(0[1-9]|1[0-2]))|(201[0-4]-(0[1-9]|1[0-2]))|(2015-(0[1-6]))\"\n",
    "    for url in all_urls:\n",
    "        # Check if the URL belongs to yellow taxi trip data\n",
    "        if re.search(pattern, url):\n",
    "            # Check if the URL belongs to the time range for the project\n",
    "            if re.search(time_pattern, url):\n",
    "                all_parquet_urls.append(url)\n",
    "            \n",
    "    return all_parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outside_trip(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove the trip records outside the defined region.\"\"\"\n",
    "    # Obtain the coordinate limits\n",
    "    southlimit, westlimit = NEW_YORK_BOX_COORDS[0]\n",
    "    northlimit, eastlimit = NEW_YORK_BOX_COORDS[1]\n",
    "    # Remove the trips outside the location \n",
    "    df = df[(df['pickup_longitude'] >= westlimit) & (df['pickup_longitude'] <= eastlimit)]\n",
    "    df = df[(df['pickup_latitude'] >= southlimit) & (df['pickup_latitude'] <= northlimit)]\n",
    "\n",
    "    df = df[(df['dropoff_longitude'] >= westlimit) & (df['dropoff_longitude'] <= eastlimit)]\n",
    "    df = df[(df['dropoff_latitude'] >= southlimit) & (df['dropoff_latitude'] <= northlimit)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coords_from_zones(dataframe):\n",
    "    \"\"\"Generate the coordinates from zone IDs in a DataFrame.\"\"\"\n",
    "    loaded_taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    southlimit, westlimit = NEW_YORK_BOX_COORDS[0]\n",
    "    northlimit, eastlimit = NEW_YORK_BOX_COORDS[1]\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        pickup_zoneid = row['pickup_zoneid']\n",
    "        dropoff_zoneid = row['dropoff_zoneid']\n",
    "\n",
    "        pickup_coords = lookup_coords_for_taxi_zone_id(pickup_zoneid, loaded_taxi_zones)\n",
    "        \n",
    "        # define the initial bearing\n",
    "        direction = 0\n",
    "        # check if pickup and dropoff zones are the same\n",
    "        if pickup_zoneid == dropoff_zoneid:\n",
    "            # generate dropoff coordinates using distance and bearing\n",
    "            dropoff_coords = distance(\n",
    "                miles=row['trip_distance']).destination(pickup_coords[::-1], bearing=direction)[1::-1]\n",
    "        else:\n",
    "            # generate dropoff coordinates using dropoff zone ID\n",
    "            dropoff_coords = lookup_coords_for_taxi_zone_id(dropoff_zoneid, loaded_taxi_zones)\n",
    "        \n",
    "        # check if dropoff coordinates fall outside the defined box\n",
    "        while not ((westlimit <= dropoff_coords[0] <= eastlimit) and \n",
    "                   (southlimit <= dropoff_coords[1] <= northlimit)):\n",
    "            # Generate new dropoff coordinates by changing the bearing\n",
    "            direction += 90\n",
    "            # If all four bearings do not work, drop this record instead\n",
    "            if direction == 360:\n",
    "                break\n",
    "            dropoff_coords = distance(\n",
    "                miles=row['trip_distance']).destination(pickup_coords, bearing=direction)[1::-1]\n",
    "        \n",
    "        # update the dataframe with the generated coordinates\n",
    "        if direction != 360:\n",
    "            dataframe.loc[index, 'pickup_longitude'] = pickup_coords[0]\n",
    "            dataframe.loc[index, 'pickup_latitude'] = pickup_coords[1]\n",
    "            dataframe.loc[index, 'dropoff_longitude'] = dropoff_coords[0]\n",
    "            dataframe.loc[index, 'dropoff_latitude'] = dropoff_coords[1]\n",
    "        else:\n",
    "            dataframe.drop(index=index, inplace=True)\n",
    "    \n",
    "    # Drop the unnecessary columns\n",
    "    dataframe.drop(['trip_distance', 'pickup_zoneid', 'dropoff_zoneid'], axis=1, inplace=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_df_2009_to_2010(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the yellow taxi data from 2009 to 2010.\"\"\"\n",
    "    # Normalize the column names\n",
    "    dataframe.columns = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "                         'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                         'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', \n",
    "                         'payment_type', 'fare_amount', 'surcharge', 'mta_tax', 'tip_amount', \n",
    "                         'tolls_amount', 'total_amount']\n",
    "    \n",
    "    # Remove the trips outside the required coordinate box\n",
    "    dataframe = remove_outside_trip(dataframe)\n",
    "    # Remove the trips with zero passenger count\n",
    "    dataframe = dataframe[dataframe['passenger_count'] != 0]\n",
    "    # Remove the trips without a fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] != 0]\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[dataframe['trip_distance'] != 0]\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] != dataframe['dropoff_longitude']) \n",
    "                          & (dataframe['pickup_latitude'] != dataframe['dropoff_latitude'])]\n",
    "    \n",
    "    # Sample the taxi data at a appropriate number\n",
    "    dataframe = dataframe.sample(n = 2500, random_state=1)\n",
    "    \n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "                       'dropoff_longitude', 'dropoff_latitude', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "    \n",
    "    # Transform the pickup datetime column from strings to datetime\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'],\n",
    "                                                  format='%Y-%m-%d %H:%M:%S') \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f900c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_df_2011_to_2015(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the yellow taxi data from 2011 to 2015.\"\"\"\n",
    "    # Normalize the column names\n",
    "    dataframe.columns = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "                         'trip_distance', 'rate_code', 'store_and_fwd_flag', 'pickup_zoneid',\n",
    "                         'dropoff_zoneid', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax',\n",
    "                         'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "                         'congestion_surcharge', 'airport_fee']\n",
    "    \n",
    "    # Remove the trips outside the 1-263 zones\n",
    "    dataframe = dataframe[dataframe['pickup_zoneid'] != 264]\n",
    "    dataframe = dataframe[dataframe['pickup_zoneid'] != 265]\n",
    "    dataframe = dataframe[dataframe['dropoff_zoneid'] != 264]\n",
    "    dataframe = dataframe[dataframe['dropoff_zoneid'] != 265]\n",
    "    # Remove the trips with zero passenger count\n",
    "    dataframe = dataframe[dataframe['passenger_count'] != 0]\n",
    "    # Remove the trips without a fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] != 0]\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[dataframe['trip_distance'] != 0]\n",
    "    \n",
    "    # Sample the taxi data at a appropriate number\n",
    "    dataframe = dataframe.sample(n = 2500, random_state=1)\n",
    "    \n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['pickup_datetime', 'trip_distance',\n",
    "                       'pickup_zoneid', 'dropoff_zoneid', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "    \n",
    "    # Generate the coordinates from zone IDs\n",
    "    dataframe = generate_coords_from_zones(dataframe)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean the parquet file for the URL, return it as a DataFrame.\"\"\"\n",
    "    # Programmatically download needed data if not exists\n",
    "    dataframe = pd.DataFrame()\n",
    "    \n",
    "    time_pattern = r\"(2009-(0[1-9]|1[0-2]))|(201[0-4]-(0[1-9]|1[0-2]))|(2015-(0[1-6]))\"\n",
    "    time = \"\"\n",
    "    \n",
    "    if re.search(time_pattern, url):\n",
    "        time = re.search(time_pattern, url).group(0)\n",
    "        file_path = f\"{TAXI_DIR}/yellow_taxi_{time}.parquet\"\n",
    "        \n",
    "        # Check if the parquet file has already been downloaded\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File {file_path} already exists.\")\n",
    "            dataframe = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        else:\n",
    "            # If not, download the file from the given URL\n",
    "            print(f\"File {file_path} does not exist. Downloading...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                print(f\"File {file_path} downloaded successfully.\")\n",
    "            dataframe = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    \n",
    "    # Clean the dataframe\n",
    "    if re.search(r\"2009|2010\", time):\n",
    "        dataframe_cleaned = clean_taxi_df_2009_to_2010(dataframe)\n",
    "    else:\n",
    "        dataframe_cleaned = clean_taxi_df_2011_to_2015(dataframe)\n",
    "    \n",
    "    return dataframe_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls: list) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess and concatenate all the data, return them as a DataFrame.\"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    # Iterate the URLs and obtain the dataframe for each month\n",
    "    for parquet_url in parquet_urls:\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # Add the 'distance' column\n",
    "        dataframe = add_distance_column(dataframe)\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "\n",
    "    # Create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() -> pd.DataFrame:\n",
    "    \"\"\"Scrap the yellow taxi data and return the result as a DataFrame.\"\"\"\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean the Uber data and return it as a DataFrame.\"\"\"\n",
    "    columns_to_keep = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "                       'dropoff_longitude', 'dropoff_latitude']\n",
    "    dataframe = pd.read_csv(csv_file, usecols = columns_to_keep)\n",
    "    # Transform the pickup datetime column from strings to datetime\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'],\n",
    "                                                  format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    dataframe['pickup_datetime'] = dataframe['pickup_datetime'].dt.tz_convert(None)\n",
    "    \n",
    "    # Remove the trips outside the defined coordinate box\n",
    "    dataframe = remove_outside_trip(dataframe)\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] != dataframe['dropoff_longitude']) \n",
    "                          & (dataframe['pickup_latitude'] != dataframe['dropoff_latitude'])]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame:\n",
    "    \"\"\"Return the processed Uber data as a DataFrame.\"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    # Add the 'distance' column\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
