{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a622c55a",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "## IEOR E4501 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import distance\n",
    "import requests\n",
    "import re\n",
    "import sqlalchemy as db\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "from typing import List, Union, Any, Dict,  Tuple\n",
    "from matplotlib.container import BarContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "TAXI_DIR = \"data/taxi\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"data/uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"data/weather\"\n",
    "\n",
    "CRS = 4326 # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "    print(f\"Folder {QUERY_DIRECTORY} created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder {QUERY_DIRECTORY} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the TAXI_DIR exists\n",
    "try:\n",
    "    os.mkdir(TAXI_DIR)\n",
    "    print(f\"Folder {TAXI_DIR} created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder {TAXI_DIR} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the shapefile and return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shapefile : str\n",
    "        The relative path of the shape file including zone IDs and geometries.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "    \n",
    "    \"\"\"\n",
    "    loaded_taxi_zones = gpd.read_file(shapefile)\n",
    "    loaded_taxi_zones = loaded_taxi_zones[['OBJECTID', 'geometry']].set_index('OBJECTID')\n",
    "    # Transform geometries to the new coordinate reference system 4326\n",
    "    loaded_taxi_zones = loaded_taxi_zones.to_crs(CRS)\n",
    "    \n",
    "    return loaded_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id: int,\n",
    "                                   loaded_taxi_zones: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Given the zone ID and return the corresponding centroid coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zone_loc_id : int\n",
    "        The zone ID which needs to be searched.\n",
    "    loaded_taxi_zones : pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the geometries and corresponding zone ID.\n",
    "\n",
    "    \"\"\"\n",
    "    geometry = loaded_taxi_zones.loc[zone_loc_id, 'geometry']\n",
    "    # Obtain the approximate coordinates by the centroid location\n",
    "    longitude = geometry.centroid.x\n",
    "    latitude = geometry.centroid.y\n",
    "    coords = (longitude, latitude)\n",
    "    \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord: tuple, to_coord: tuple) -> float:\n",
    "    \"\"\"Given the coordinates and return the distance between them.\n",
    "    \n",
    "    This function utilizes the Haversine formula to calculate the distance\n",
    "    between two coordinates on Earth's surface.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    from_coord : tuple of float\n",
    "        A tuple containing the longitude and latitude of the starting point, expressed in degrees.\n",
    "    to_coord : tuple of float\n",
    "        A tuple containing the longitude and latitude of the destination point, expressed in degrees.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The distance between the two coordinates, in kilometers.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input coordinates from degrees to radians\n",
    "    from_lon, from_lat = math.radians(from_coord[0]), math.radians(from_coord[1])\n",
    "    to_lon, to_lat = math.radians(to_coord[0]), math.radians(to_coord[1])\n",
    "    # Calculate the differences in latitude and longitude\n",
    "    delta_lon = to_lon - from_lon\n",
    "    delta_lat = to_lat - from_lat\n",
    "    # Apply the Haversine formula to calculate the distance\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(from_lat) * math.cos(to_lat) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = 6371 * c # earth's radius is assumed to be 6371 kilometers\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add the 'distance' column to the dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including the new calculated column 'distance'.\n",
    "    \n",
    "    \"\"\"\n",
    "    dataframe['distance'] = dataframe.apply(lambda x:calculate_distance_with_coords(\n",
    "                                (x['pickup_longitude'],x['pickup_latitude']),\n",
    "                                (x['dropoff_longitude'],x['dropoff_latitude'])), axis=1)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c0aeb",
   "metadata": {},
   "source": [
    "### Remove outside trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outside_trip(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove the trip records outside the defined region.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after removing all trip records outside the defined region.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Obtain the coordinate limits\n",
    "    southlimit, westlimit = NEW_YORK_BOX_COORDS[0]\n",
    "    northlimit, eastlimit = NEW_YORK_BOX_COORDS[1]\n",
    "    # Remove the trips outside the location \n",
    "    df = df[(df['pickup_longitude'] >= westlimit) & (df['pickup_longitude'] <= eastlimit)]\n",
    "    df = df[(df['pickup_latitude'] >= southlimit) & (df['pickup_latitude'] <= northlimit)]\n",
    "\n",
    "    df = df[(df['dropoff_longitude'] >= westlimit) & (df['dropoff_longitude'] <= eastlimit)]\n",
    "    df = df[(df['dropoff_latitude'] >= southlimit) & (df['dropoff_latitude'] <= northlimit)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page: str) -> list:\n",
    "    \"\"\"Scrap the URLs from the given page and return them as a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    taxi_page : str\n",
    "        The URL of the target page.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of URLs scraped from the given page.\n",
    "        \n",
    "    \"\"\"\n",
    "    all_urls = list()\n",
    "    \n",
    "    content = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(content.text, 'lxml')\n",
    "    # Find all the URLs in the page\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        all_urls.append(link.get('href'))\n",
    "        \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_parquet_urls(all_urls: list) -> list:\n",
    "    \"\"\"Find the URLs for yellow taxi data and return them as a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_urls : str\n",
    "        A list of URLs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of URLs for yellow taxi parquet files from 2019-01 to 2015-06.\n",
    "        \n",
    "    \"\"\"\n",
    "    all_parquet_urls = list()\n",
    "    \n",
    "    pattern = r\".*yellow_tripdata.*parquet\\Z\"\n",
    "    time_pattern = r\"(2009-(0[1-9]|1[0-2]))|(201[0-4]-(0[1-9]|1[0-2]))|(2015-(0[1-6]))\"\n",
    "    for url in all_urls:\n",
    "        # Check if the URL belongs to yellow taxi trip data\n",
    "        if re.search(pattern, url):\n",
    "            # Check if the URL belongs to the time range for the project\n",
    "            if re.search(time_pattern, url):\n",
    "                all_parquet_urls.append(url)\n",
    "            \n",
    "    return all_parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coords_from_zones(dataframe):\n",
    "    \"\"\"Generate the coordinates from zone IDs in a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe which needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after transforming zone IDs to longitude and latitude coordinates.\n",
    "    \n",
    "    \"\"\"\n",
    "    loaded_taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    southlimit, westlimit = NEW_YORK_BOX_COORDS[0]\n",
    "    northlimit, eastlimit = NEW_YORK_BOX_COORDS[1]\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        pickup_zoneid = row['pickup_zoneid']\n",
    "        dropoff_zoneid = row['dropoff_zoneid']\n",
    "\n",
    "        pickup_coords = lookup_coords_for_taxi_zone_id(pickup_zoneid, loaded_taxi_zones)\n",
    "        \n",
    "        # define the initial bearing\n",
    "        direction = 0\n",
    "        # check if pickup and dropoff zones are the same\n",
    "        if pickup_zoneid == dropoff_zoneid:\n",
    "            # generate dropoff coordinates using distance and bearing\n",
    "            dropoff_coords = distance(\n",
    "                miles=row['trip_distance']).destination(pickup_coords[::-1], bearing=direction)[1::-1]\n",
    "        else:\n",
    "            # generate dropoff coordinates using dropoff zone ID\n",
    "            dropoff_coords = lookup_coords_for_taxi_zone_id(dropoff_zoneid, loaded_taxi_zones)\n",
    "        \n",
    "        # check if dropoff coordinates fall outside the defined box\n",
    "        while not ((westlimit <= dropoff_coords[0] <= eastlimit) and \n",
    "                   (southlimit <= dropoff_coords[1] <= northlimit)):\n",
    "            # Generate new dropoff coordinates by changing the bearing\n",
    "            direction += 90\n",
    "            # If all four bearings do not work, drop this record instead\n",
    "            if direction == 360:\n",
    "                break\n",
    "            dropoff_coords = distance(\n",
    "                miles=row['trip_distance']).destination(pickup_coords, bearing=direction)[1::-1]\n",
    "        \n",
    "        # update the dataframe with the generated coordinates\n",
    "        if direction != 360:\n",
    "            dataframe.loc[index, 'pickup_longitude'] = pickup_coords[0]\n",
    "            dataframe.loc[index, 'pickup_latitude'] = pickup_coords[1]\n",
    "            dataframe.loc[index, 'dropoff_longitude'] = dropoff_coords[0]\n",
    "            dataframe.loc[index, 'dropoff_latitude'] = dropoff_coords[1]\n",
    "        else:\n",
    "            dataframe.drop(index=index, inplace=True)\n",
    "    \n",
    "    # Drop the unnecessary columns\n",
    "    dataframe.drop(['trip_distance', 'pickup_zoneid', 'dropoff_zoneid'], axis=1, inplace=True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_df_2009_to_2010(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the yellow taxi data from 2009 to 2010.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe with records from 2009 to 2010.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after normalizing column names, removing invalid data, and sampling.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Normalize the column names\n",
    "    dataframe.columns = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "                         'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                         'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', \n",
    "                         'payment_type', 'fare_amount', 'surcharge', 'mta_tax', 'tip_amount', \n",
    "                         'tolls_amount', 'total_amount']\n",
    "    \n",
    "    # Remove the trips outside the required coordinate box\n",
    "    dataframe = remove_outside_trip(dataframe)\n",
    "    # Remove the trips with zero passenger count\n",
    "    dataframe = dataframe[dataframe['passenger_count'] != 0]\n",
    "    # Remove the trips without a fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] != 0]\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[dataframe['trip_distance'] != 0]\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] != dataframe['dropoff_longitude']) \n",
    "                          & (dataframe['pickup_latitude'] != dataframe['dropoff_latitude'])]\n",
    "    \n",
    "    # Sample the taxi data at a appropriate number\n",
    "    dataframe = dataframe.sample(n = 2500, random_state=1)\n",
    "    \n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "                       'dropoff_longitude', 'dropoff_latitude', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "    \n",
    "    # Transform the pickup datetime column from strings to datetime\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'],\n",
    "                                                  format='%Y-%m-%d %H:%M:%S') \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f900c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_df_2011_to_2015(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean the yellow taxi data from 2011 to 2015.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe with records from 2011 to 2015.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after normalizing column names, removing invalid data, and sampling.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Normalize the column names\n",
    "    dataframe.columns = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
    "                         'trip_distance', 'rate_code', 'store_and_fwd_flag', 'pickup_zoneid',\n",
    "                         'dropoff_zoneid', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax',\n",
    "                         'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "                         'congestion_surcharge', 'airport_fee']\n",
    "    \n",
    "    # Remove the trips outside the 1-263 zones\n",
    "    dataframe = dataframe[dataframe['pickup_zoneid'] != 264]\n",
    "    dataframe = dataframe[dataframe['pickup_zoneid'] != 265]\n",
    "    dataframe = dataframe[dataframe['dropoff_zoneid'] != 264]\n",
    "    dataframe = dataframe[dataframe['dropoff_zoneid'] != 265]\n",
    "    # Remove the trips with zero passenger count\n",
    "    dataframe = dataframe[dataframe['passenger_count'] != 0]\n",
    "    # Remove the trips without a fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] != 0]\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[dataframe['trip_distance'] != 0]\n",
    "    \n",
    "    # Sample the taxi data at a appropriate number\n",
    "    dataframe = dataframe.sample(n = 2500, random_state=1)\n",
    "    \n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = ['pickup_datetime', 'trip_distance',\n",
    "                       'pickup_zoneid', 'dropoff_zoneid', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "    \n",
    "    # Generate the coordinates from zone IDs\n",
    "    dataframe = generate_coords_from_zones(dataframe)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean the parquet file for the URL, return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL for the parquet file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after loading and cleaning the parquet file from the given URL.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Programmatically download needed data if not exists\n",
    "    dataframe = pd.DataFrame()\n",
    "    \n",
    "    time_pattern = r\"(2009-(0[1-9]|1[0-2]))|(201[0-4]-(0[1-9]|1[0-2]))|(2015-(0[1-6]))\"\n",
    "    time = \"\"\n",
    "    \n",
    "    if re.search(time_pattern, url):\n",
    "        time = re.search(time_pattern, url).group(0)\n",
    "        file_path = f\"{TAXI_DIR}/yellow_taxi_{time}.parquet\"\n",
    "        \n",
    "        # Check if the parquet file has already been downloaded\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File {file_path} already exists.\")\n",
    "            dataframe = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        else:\n",
    "            # If not, download the file from the given URL\n",
    "            print(f\"File {file_path} does not exist. Downloading...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                print(f\"File {file_path} downloaded successfully.\")\n",
    "            dataframe = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    \n",
    "    # Clean the dataframe\n",
    "    if re.search(r\"2009|2010\", time):\n",
    "        dataframe_cleaned = clean_taxi_df_2009_to_2010(dataframe)\n",
    "    else:\n",
    "        dataframe_cleaned = clean_taxi_df_2011_to_2015(dataframe)\n",
    "    \n",
    "    return dataframe_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls: list) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess and concatenate all the data, return them as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parquet_urls : list\n",
    "        A list of URLs for parquet files of yellow taxi data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after preprocessing and concatenating all the parquet file data.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    # Iterate the URLs and obtain the dataframe for each month\n",
    "    for parquet_url in parquet_urls:\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # Add the 'distance' column\n",
    "        dataframe = add_distance_column(dataframe)\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "\n",
    "    # Create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() -> pd.DataFrame:\n",
    "    \"\"\"Scrap the yellow taxi data and return the result as a DataFrame.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe including all cleaned and sampled records for yellow taxi data.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sampled yellow taxi data as a CSV file\n",
    "taxi_data.to_csv('data/taxi/yellow_taxi_sampled.csv', index=False)\n",
    "# Load the sampled yellow taxi data directly\n",
    "taxi_data = pd.read_csv('data/taxi/yellow_taxi_sampled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean the Uber data and return it as a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_file : str\n",
    "        The relative path of the CSV file of Uber data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after loading and cleaning all the Uber data.\n",
    "    \n",
    "    \"\"\"\n",
    "    columns_to_keep = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "                       'dropoff_longitude', 'dropoff_latitude']\n",
    "    dataframe = pd.read_csv(csv_file, usecols = columns_to_keep)\n",
    "    # Transform the pickup datetime column from strings to datetime\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'],\n",
    "                                                  format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    dataframe['pickup_datetime'] = dataframe['pickup_datetime'].dt.tz_convert(None)\n",
    "    \n",
    "    # Remove the trips outside the defined coordinate box\n",
    "    dataframe = remove_outside_trip(dataframe)\n",
    "    # Remove the trips with no distance between pickup and dropoff locations\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] != dataframe['dropoff_longitude']) \n",
    "                          & (dataframe['pickup_latitude'] != dataframe['dropoff_latitude'])]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame:\n",
    "    \"\"\"Return the processed Uber data as a DataFrame.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe after preprocessing all the Uber data and adding column 'distance'.\n",
    "    \n",
    "    \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    # Add the 'distance' column\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_invalid_rows_for_HourlyWindSpeed(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows from the input dataframe if the DATE column value is not 23:59\n",
    "    and the HourlyWindSpeed column value is not a float or is NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Input dataframe with 'DATE' and 'HourlyWindSpeed' columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataframe after dropping invalid rows.\n",
    "    \"\"\"\n",
    "    # Filter rows with DATE values not equal to 23:59\n",
    "    not_2359_rows = (dataframe['DATE'].dt.hour != 23) | (dataframe['DATE'].dt.minute != 59)\n",
    "    \n",
    "    # Check if the HourlyWindSpeed is of type float and not NaN\n",
    "    is_float_and_not_nan = dataframe['HourlyWindSpeed'].apply(lambda x: isinstance(x, float) and not pd.isna(x))\n",
    "    \n",
    "    # Combine both conditions using bitwise AND\n",
    "    invalid_rows = not_2359_rows & ~is_float_and_not_nan\n",
    "\n",
    "    # Drop the invalid rows\n",
    "    dataframe = dataframe[~invalid_rows]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_missing_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a DataFrame containing missing daily records.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing weather data from January 2009 to June 2015.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with missing daily records.\n",
    "    \"\"\"\n",
    "    # Find the minimum and maximum dates in the input DataFrame\n",
    "    min_date = df['DATE'].min().normalize()\n",
    "    max_date = df['DATE'].max().normalize()\n",
    "    # Create a date range from the minimum to maximum date\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    # Generate missing daily records by checking if a record exists for each date\n",
    "    missing_dates = [\n",
    "        {'DATE': date.replace(hour=23, minute=59, second=0)}\n",
    "        for date in date_range\n",
    "        if not ((df['DATE'] == date.replace(hour=23, minute=59, second=0)).any())\n",
    "    ]\n",
    "    return pd.DataFrame(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828d910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_daily_wind_speed(new_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Fill missing daily wind speed values by calculating the mean hourly wind speed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    new_df : pd.DataFrame\n",
    "        The input DataFrame containing weather data with missing daily wind speed values.\n",
    "    \"\"\"\n",
    "    # Iterate through the rows of the DataFrame with missing daily wind speed values\n",
    "    for index, row in new_df.loc[new_df['DATE'].dt.strftime('%H:%M') == '23:59'].iterrows():\n",
    "        if pd.isna(row['DailyAverageWindSpeed']):\n",
    "            date = row['DATE'].date()\n",
    "            # Calculate the mean hourly wind speed for the current date\n",
    "            hourly_wind_speed_mean = new_df.loc[\n",
    "                (new_df['DATE'].dt.date == date) & (new_df['DATE'] != row['DATE']) & (~new_df['HourlyWindSpeed'].isna()),\n",
    "                'HourlyWindSpeed'\n",
    "            ].mean()\n",
    "            # Replace the missing daily wind speed value with the calculated mean\n",
    "            new_df.loc[index, 'DailyAverageWindSpeed'] = hourly_wind_speed_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean the CSV files, return them as a list of DataFrames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        The directory path containing the CSV files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[pd.DataFrame]\n",
    "        A list of cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    # List all CSV files in the directory\n",
    "    csv_files = [entry.name for entry in os.scandir(directory)\n",
    "        if entry.name.endswith('.csv') and entry.is_file()]\n",
    "    dfs = []\n",
    "    # Choose useful columns for the coming analysis\n",
    "    columns_to_keep = [\n",
    "        'DATE', 'HourlyPrecipitation', 'HourlyWindSpeed',\n",
    "        'DailyAverageWindSpeed', 'Sunrise', 'Sunset'\n",
    "    ]\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Join the directory path and CSV file name to form the full file path\n",
    "        file_path = os.path.join(directory, csv_file)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, usecols=columns_to_keep, parse_dates=['DATE'], engine='python')\n",
    "        \n",
    "        # Filter the DataFrame by date range\n",
    "        df = df[(df['DATE'] >= '2009-01-01 00:00:00') & (df['DATE'] <= '2015-06-30 23:59:59')]\n",
    "        \n",
    "        # Drop rows that do not meet the requirement of 'HourlyWindSpeed'.\n",
    "        df = drop_invalid_rows_for_HourlyWindSpeed(df)\n",
    "        \n",
    "        # Generate a DataFrame with missing dates\n",
    "        missing_df = generate_missing_dates(df)\n",
    "        \n",
    "        # Combine the original DataFrame and the missing dates DataFrame\n",
    "        new_df = pd.concat([df, missing_df], ignore_index=True)\n",
    "        # Sort the new DataFrame by date\n",
    "        new_df.sort_values(by='DATE', inplace=True)\n",
    "        \n",
    "        # Fill in missing daily wind speed values\n",
    "        fill_missing_daily_wind_speed(new_df)\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list of DataFrames\n",
    "        dfs.append(new_df)\n",
    "        \n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hourly_precipitation(hourly_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the 'HourlyPrecipitation' column of the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hourly_data : pd.DataFrame\n",
    "        Input DataFrame containing hourly weather data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing hourly data.\n",
    "    \"\"\"\n",
    "    # Remove 's' from the HourlyPrecipitation values, replace 'T' with 0, and fill missing values with 0\n",
    "    hourly_data['HourlyPrecipitation'] = hourly_data['HourlyPrecipitation'].str.replace('s', '')\n",
    "    hourly_data['HourlyPrecipitation'] = hourly_data['HourlyPrecipitation'].replace('T', 0)\n",
    "    hourly_data['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    # Convert the HourlyPrecipitation column to numeric type\n",
    "    hourly_data['HourlyPrecipitation'] = pd.to_numeric(hourly_data['HourlyPrecipitation'])\n",
    "    return hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd1b2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_hourly_data(hourly_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate the input DataFrame's data to remove multiple reports within an hour.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hourly_data : pd.DataFrame\n",
    "        Input DataFrame containing hourly weather data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with aggregated hourly data.\n",
    "    \"\"\"\n",
    "    # Round the datetime to the nearest hour\n",
    "    hourly_data['date'] = hourly_data['date'].dt.floor('H')\n",
    "    # Group by hour and aggregate the data by taking the maximum of hourly_precipitation and hourly_windspeed\n",
    "    return hourly_data.groupby('date').agg(\n",
    "        {'hourly_precipitation': 'max', 'hourly_windspeed': 'max'}\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be09c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the DataFrame and return only hourly weather data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_file : pd.DataFrame\n",
    "        Input DataFrame containing weather data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with aggregated hourly data.\n",
    "    \"\"\"\n",
    "    # Split the input DataFrame into daily and hourly data\n",
    "    daily_data = csv_file[(csv_file['DATE'].dt.hour == 23) & (csv_file['DATE'].dt.minute == 59)]\n",
    "    hourly_data = csv_file.drop(daily_data.index)\n",
    "    # Select only relevant columns for further processing\n",
    "    columns_to_keep = ['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']\n",
    "    hourly_data = hourly_data[columns_to_keep]\n",
    "\n",
    "    # Clean the hourly precipitation and wind speed data\n",
    "    hourly_data = clean_hourly_precipitation(hourly_data)\n",
    "    # Clean the 'HourlyWindSpeed' column\n",
    "    hourly_data['HourlyWindSpeed'].fillna(0, inplace=True)\n",
    "    # Normalize the column names\n",
    "    hourly_data.columns = ['date', 'hourly_precipitation', 'hourly_windspeed']\n",
    "    # Aggregate the hourly data to remove multiple reports within an hour\n",
    "    result = aggregate_hourly_data(hourly_data)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a15f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_time_string(value: float) -> str:\n",
    "    \"\"\"\n",
    "    Transform the float value to time string in HH:MM format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    value : float\n",
    "        Float value representing time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Formatted time string in HH:MM format.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    else:\n",
    "        # Extract hours and minutes from the float value\n",
    "        hours = int(value // 100)\n",
    "        minutes = int(value % 100)\n",
    "    # Return formatted time string\n",
    "    return f\"{hours:02d}:{minutes:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de8e4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_datetime(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert 'Sunrise' and 'Sunset' time strings to datetime objects using the date from the 'DATE' column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : Dict[str, Any]\n",
    "        A dictionary representing a row in the DataFrame containing 'DATE', 'Sunrise', and 'Sunset' columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        The modified row with 'Sunrise' and 'Sunset' columns converted to datetime objects.\n",
    "    \"\"\"\n",
    "    # Extract the date from the 'DATE' column\n",
    "    date = row['DATE'].date()\n",
    "    \n",
    "    # Check if the 'Sunrise' value is not None\n",
    "    if row['Sunrise'] is not None:\n",
    "        # Convert the 'Sunrise' time string to a time object\n",
    "        sunrise_time = pd.to_datetime(row['Sunrise'], format=\"%H:%M\").time()\n",
    "        # Combine the date and sunrise_time to create a datetime object\n",
    "        row['Sunrise'] = pd.to_datetime(f\"{date} {sunrise_time}\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Check if the 'Sunset' value is not None\n",
    "    if row['Sunset'] is not None:\n",
    "        # Convert the 'Sunset' time string to a time object\n",
    "        sunset_time = pd.to_datetime(row['Sunset'], format=\"%H:%M\").time()\n",
    "        # Combine the date and sunset_time to create a datetime object\n",
    "        row['Sunset'] = pd.to_datetime(f\"{date} {sunset_time}\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Return the modified row\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69bbbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the DataFrame and return only daily weather data.\n",
    "\n",
    "    The function processes the input DataFrame and returns a cleaned DataFrame\n",
    "    with daily weather data. The steps include extracting daily data, selecting\n",
    "    relevant columns, cleaning sunrise and sunset data, and normalizing column names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_file : pd.DataFrame\n",
    "        DataFrame containing weather data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing cleaned daily weather data.\n",
    "    \"\"\"\n",
    "    # Filter daily data based on timestamp (hour=23 and minute=59)\n",
    "    daily_data = csv_file[(csv_file['DATE'].dt.hour == 23) & (csv_file['DATE'].dt.minute == 59)]\n",
    "    # Define the columns to keep\n",
    "    columns_to_keep = ['DATE', 'DailyAverageWindSpeed', 'Sunrise', 'Sunset']\n",
    "    daily_data = daily_data[columns_to_keep]\n",
    "    \n",
    "    # Clean the  \"Sunrise\" and \"Sunset\" columns\n",
    "    # Convert 'Sunrise' and 'Sunset' columns to strings in HH:MM format\n",
    "    daily_data['Sunrise'] = daily_data['Sunrise'].apply(float_to_time_string)\n",
    "    daily_data['Sunset'] = daily_data['Sunset'].apply(float_to_time_string)\n",
    "    # Convert 'Sunrise' and 'Sunset' columns to datetime\n",
    "    daily_data = daily_data.apply(convert_time_to_datetime, axis=1)\n",
    "    \n",
    "    # Normalize the column names\n",
    "    daily_data.columns = ['date', 'daily_average_windspeed', 'sunrise', 'sunset']\n",
    "    # Round the datetime column in a DataFrame to the nearest day.\n",
    "    daily_data[\"date\"] = pd.to_datetime(daily_data[\"date\"]).dt.date\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f4444eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Load and clean weather data, return hourly and daily records as DataFrames.\n",
    "\n",
    "    The function loads weather data from CSV files, cleans the data, and returns\n",
    "    two DataFrames containing hourly and daily weather data respectively.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple containing two DataFrames: (hourly_data, daily_data).\n",
    "    \"\"\"\n",
    "    # Load weather CSV files\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "\n",
    "    # Process each CSV file\n",
    "    for csv_file in weather_csv_files:\n",
    "        # Clean hourly and daily weather data\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        # Append DataFrames to respective lists\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "\n",
    "    # Create two DataFrames with hourly and daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "\n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourly_precipitation</th>\n",
       "      <th>hourly_windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  hourly_precipitation  hourly_windspeed\n",
       "0 2009-01-01 00:00:00                   0.0              18.0\n",
       "1 2009-01-01 01:00:00                   0.0              18.0\n",
       "2 2009-01-01 02:00:00                   0.0              18.0\n",
       "3 2009-01-01 03:00:00                   0.0               8.0\n",
       "4 2009-01-01 04:00:00                   0.0              11.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_average_windspeed</th>\n",
       "      <th>sunrise</th>\n",
       "      <th>sunset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11123</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>11.041667</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>6.806452</td>\n",
       "      <td>2009-01-02 07:20:00</td>\n",
       "      <td>2009-01-02 16:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11124</th>\n",
       "      <td>2009-01-03</td>\n",
       "      <td>9.875000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11125</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>7.370370</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11126</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>6.925926</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  daily_average_windspeed             sunrise  \\\n",
       "11123  2009-01-01                11.041667                 NaT   \n",
       "55     2009-01-02                 6.806452 2009-01-02 07:20:00   \n",
       "11124  2009-01-03                 9.875000                 NaT   \n",
       "11125  2009-01-04                 7.370370                 NaT   \n",
       "11126  2009-01-05                 6.925926                 NaT   \n",
       "\n",
       "                   sunset  \n",
       "11123                 NaT  \n",
       "55    2009-01-02 16:40:00  \n",
       "11124                 NaT  \n",
       "11125                 NaT  \n",
       "11126                 NaT  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table schema\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_windspeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date DATETIME,\n",
    "    daily_average_windspeed FLOAT,\n",
    "    sunrise DATETIME,\n",
    "    sunset DATETIME\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict: Dict[str, pd.DataFrame]) -> None:\n",
    "    \"\"\"\n",
    "    Write DataFrames to their corresponding database tables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_to_df_dict : Dict[str, pd.DataFrame]\n",
    "        A dictionary where keys represent table names and values represent DataFrames to be written to those tables.\n",
    "    \"\"\"\n",
    "    # Iterate through the table name and DataFrame pairs in the dictionary\n",
    "    for table, dataframe in table_to_df_dict.items():\n",
    "        # Write the DataFrame to the corresponding table in the database\n",
    "        dataframe.to_sql(table, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
